{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e90387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal\n",
    "#To build a next word prediction model using LSTM in TensorFlow/Keras.\n",
    "\n",
    "# say the line is : \"Hi my name is Ajitesh and I am learning deep learning\"\n",
    "# toh its like \n",
    "# think in terms of making it supervised learning model \n",
    "# like predicting the next word \n",
    "\n",
    "# Input           | Ouput\n",
    "# Hi              | my\n",
    "# Hi my           | name\n",
    "# Hi my name      | is\n",
    "# Hi my name is   | Ajitesh\n",
    "# Hi my name is Ajitesh | and\n",
    "# Hi my name is Ajitesh and | I\n",
    "# Hi my name is Ajitesh and I | am\n",
    "# Hi my name is Ajitesh and I am | learning\n",
    "# Hi my name is Ajitesh and I am learning | deep\n",
    "# Hi my name is Ajitesh and I am learning deep | learning\n",
    "\n",
    "\n",
    "# convert the text into sequences of integers encoded \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data= \"\"\"Hey Ajitesh, \n",
    "This is a reminder that we've accepted your application to ETHOnline 2025. It would be great if you are able to confirm your attendance by clicking the link below and staking within the next 24 hours.\n",
    "Hacker Dashboard\n",
    "RSVP for ETHOnline 2025 here\n",
    "In order to confirm your attendance, we require you to stake a small deposit on our dashboard. This is a temporary deposit that we return after a successful hackathon participation (more info here). Let us know if this is an issue for you.\n",
    "Here are some important things for you to do after confirming your attendance:\n",
    "Check the event schedule outline on our website\n",
    "Join the event Discord (link on dashboard)!\n",
    "Keep in mind that the Code of Conduct we have in place for all ETHGlobal events\n",
    "Check out the Event Info Center → your go-to resource for the hackathon\n",
    "Say hello in #find-a-team discord channel to chat with other hackers (post staking)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89044224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey Ajitesh, \\nThis is a reminder that we've accepted your application to ETHOnline 2025. It would be great if you are able to confirm your attendance by clicking the link below and staking within the next 24 hours.\\nHacker Dashboard\\nRSVP for ETHOnline 2025 here\\nIn order to confirm your attendance, we require you to stake a small deposit on our dashboard. This is a temporary deposit that we return after a successful hackathon participation (more info here). Let us know if this is an issue for you.\\nHere are some important things for you to do after confirming your attendance:\\nCheck the event schedule outline on our website\\nJoin the event Discord (link on dashboard)!\\nKeep in mind that the Code of Conduct we have in place for all ETHGlobal events\\nCheck out the Event Info Center → your go-to resource for the hackathon\\nSay hello in #find-a-team discord channel to chat with other hackers (post staking)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df63ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c524b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 1, 'the': 2, 'a': 3, 'your': 4, 'for': 5, 'you': 6, 'in': 7, 'this': 8, 'is': 9, 'that': 10, 'attendance': 11, 'dashboard': 12, 'here': 13, 'we': 14, 'on': 15, 'event': 16, 'ethonline': 17, '2025': 18, 'if': 19, 'are': 20, 'confirm': 21, 'link': 22, 'staking': 23, 'deposit': 24, 'our': 25, 'after': 26, 'hackathon': 27, 'info': 28, 'check': 29, 'discord': 30, 'hey': 31, 'ajitesh': 32, 'reminder': 33, \"we've\": 34, 'accepted': 35, 'application': 36, 'it': 37, 'would': 38, 'be': 39, 'great': 40, 'able': 41, 'by': 42, 'clicking': 43, 'below': 44, 'and': 45, 'within': 46, 'next': 47, '24': 48, 'hours': 49, 'hacker': 50, 'rsvp': 51, 'order': 52, 'require': 53, 'stake': 54, 'small': 55, 'temporary': 56, 'return': 57, 'successful': 58, 'participation': 59, 'more': 60, 'let': 61, 'us': 62, 'know': 63, 'an': 64, 'issue': 65, 'some': 66, 'important': 67, 'things': 68, 'do': 69, 'confirming': 70, 'schedule': 71, 'outline': 72, 'website': 73, 'join': 74, 'keep': 75, 'mind': 76, 'code': 77, 'of': 78, 'conduct': 79, 'have': 80, 'place': 81, 'all': 82, 'ethglobal': 83, 'events': 84, 'out': 85, 'center': 86, '→': 87, 'go': 88, 'resource': 89, 'say': 90, 'hello': 91, 'find': 92, 'team': 93, 'channel': 94, 'chat': 95, 'with': 96, 'other': 97, 'hackers': 98, 'post': 99}\n",
      "[31, 32]\n",
      "[8, 9]\n",
      "[8, 9, 3]\n",
      "[8, 9, 3, 33]\n",
      "[8, 9, 3, 33, 10]\n",
      "[8, 9, 3, 33, 10, 34]\n",
      "[8, 9, 3, 33, 10, 34, 35]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45, 23]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45, 23, 46]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45, 23, 46, 2]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45, 23, 46, 2, 47]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45, 23, 46, 2, 47, 48]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45, 23, 46, 2, 47, 48, 49]\n",
      "[50, 12]\n",
      "[51, 5]\n",
      "[51, 5, 17]\n",
      "[51, 5, 17, 18]\n",
      "[51, 5, 17, 18, 13]\n",
      "[7, 52]\n",
      "[7, 52, 1]\n",
      "[7, 52, 1, 21]\n",
      "[7, 52, 1, 21, 4]\n",
      "[7, 52, 1, 21, 4, 11]\n",
      "[7, 52, 1, 21, 4, 11, 14]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19, 8]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19, 8, 9]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19, 8, 9, 64]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19, 8, 9, 64, 65]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19, 8, 9, 64, 65, 5]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19, 8, 9, 64, 65, 5, 6]\n",
      "[13, 20]\n",
      "[13, 20, 66]\n",
      "[13, 20, 66, 67]\n",
      "[13, 20, 66, 67, 68]\n",
      "[13, 20, 66, 67, 68, 5]\n",
      "[13, 20, 66, 67, 68, 5, 6]\n",
      "[13, 20, 66, 67, 68, 5, 6, 1]\n",
      "[13, 20, 66, 67, 68, 5, 6, 1, 69]\n",
      "[13, 20, 66, 67, 68, 5, 6, 1, 69, 26]\n",
      "[13, 20, 66, 67, 68, 5, 6, 1, 69, 26, 70]\n",
      "[13, 20, 66, 67, 68, 5, 6, 1, 69, 26, 70, 4]\n",
      "[13, 20, 66, 67, 68, 5, 6, 1, 69, 26, 70, 4, 11]\n",
      "[29, 2]\n",
      "[29, 2, 16]\n",
      "[29, 2, 16, 71]\n",
      "[29, 2, 16, 71, 72]\n",
      "[29, 2, 16, 71, 72, 15]\n",
      "[29, 2, 16, 71, 72, 15, 25]\n",
      "[29, 2, 16, 71, 72, 15, 25, 73]\n",
      "[74, 2]\n",
      "[74, 2, 16]\n",
      "[74, 2, 16, 30]\n",
      "[74, 2, 16, 30, 22]\n",
      "[74, 2, 16, 30, 22, 15]\n",
      "[74, 2, 16, 30, 22, 15, 12]\n",
      "[75, 7]\n",
      "[75, 7, 76]\n",
      "[75, 7, 76, 10]\n",
      "[75, 7, 76, 10, 2]\n",
      "[75, 7, 76, 10, 2, 77]\n",
      "[75, 7, 76, 10, 2, 77, 78]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80, 7]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80, 7, 81]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80, 7, 81, 5]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80, 7, 81, 5, 82]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80, 7, 81, 5, 82, 83]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80, 7, 81, 5, 82, 83, 84]\n",
      "[29, 85]\n",
      "[29, 85, 2]\n",
      "[29, 85, 2, 16]\n",
      "[29, 85, 2, 16, 28]\n",
      "[29, 85, 2, 16, 28, 86]\n",
      "[29, 85, 2, 16, 28, 86, 87]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4, 88]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4, 88, 1]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4, 88, 1, 89]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4, 88, 1, 89, 5]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4, 88, 1, 89, 5, 2]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4, 88, 1, 89, 5, 2, 27]\n",
      "[90, 91]\n",
      "[90, 91, 7]\n",
      "[90, 91, 7, 92]\n",
      "[90, 91, 7, 92, 3]\n",
      "[90, 91, 7, 92, 3, 93]\n",
      "[90, 91, 7, 92, 3, 93, 30]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1, 95]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1, 95, 96]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1, 95, 96, 97]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1, 95, 96, 97, 98]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1, 95, 96, 97, 98, 99]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1, 95, 96, 97, 98, 99, 23]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "#print encoded text\n",
    "input_sequences = []\n",
    "for line in data.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# print (input_sequences) in the format of list of lists and \\n after every element\n",
    "for seq in input_sequences:\n",
    "    print(seq, end='\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06fb77cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 43\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 31 32]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 8 9]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 8 9 3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40 19]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40 19  6]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40 19  6 20]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8\n",
      "  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40 19  6 20 41]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9\n",
      "  3 33 10 34 35  4 36  1 17 18 37 38 39 40 19  6 20 41  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3\n",
      " 33 10 34 35  4 36  1 17 18 37 38 39 40 19  6 20 41  1 21]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33\n",
      " 10 34 35  4 36  1 17 18 37 38 39 40 19  6 20 41  1 21  4]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10\n",
      " 34 35  4 36  1 17 18 37 38 39 40 19  6 20 41  1 21  4 11]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34\n",
      " 35  4 36  1 17 18 37 38 39 40 19  6 20 41  1 21  4 11 42]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35\n",
      "  4 36  1 17 18 37 38 39 40 19  6 20 41  1 21  4 11 42 43]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4\n",
      " 36  1 17 18 37 38 39 40 19  6 20 41  1 21  4 11 42 43  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36\n",
      "  1 17 18 37 38 39 40 19  6 20 41  1 21  4 11 42 43  2 22]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1\n",
      " 17 18 37 38 39 40 19  6 20 41  1 21  4 11 42 43  2 22 44]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17\n",
      " 18 37 38 39 40 19  6 20 41  1 21  4 11 42 43  2 22 44 45]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18\n",
      " 37 38 39 40 19  6 20 41  1 21  4 11 42 43  2 22 44 45 23]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37\n",
      " 38 39 40 19  6 20 41  1 21  4 11 42 43  2 22 44 45 23 46]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38\n",
      " 39 40 19  6 20 41  1 21  4 11 42 43  2 22 44 45 23 46  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39\n",
      " 40 19  6 20 41  1 21  4 11 42 43  2 22 44 45 23 46  2 47]\n",
      "[ 0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40\n",
      " 19  6 20 41  1 21  4 11 42 43  2 22 44 45 23 46  2 47 48]\n",
      "[ 0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40 19\n",
      "  6 20 41  1 21  4 11 42 43  2 22 44 45 23 46  2 47 48 49]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 50 12]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 51  5]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 51  5 17]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 51  5 17 18]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 51  5 17 18 13]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7\n",
      " 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52\n",
      "  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1\n",
      " 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21\n",
      "  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4\n",
      " 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10 14]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11\n",
      " 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10 14 57]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14\n",
      " 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10 14 57 26]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53\n",
      "  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10 14 57 26  3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6\n",
      "  1 54  3 55 24 15 25 12  8  9  3 56 24 10 14 57 26  3 58]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1\n",
      " 54  3 55 24 15 25 12  8  9  3 56 24 10 14 57 26  3 58 27]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54\n",
      "  3 55 24 15 25 12  8  9  3 56 24 10 14 57 26  3 58 27 59]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3\n",
      " 55 24 15 25 12  8  9  3 56 24 10 14 57 26  3 58 27 59 60]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55\n",
      " 24 15 25 12  8  9  3 56 24 10 14 57 26  3 58 27 59 60 28]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24\n",
      " 15 25 12  8  9  3 56 24 10 14 57 26  3 58 27 59 60 28 13]\n",
      "[ 0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15\n",
      " 25 12  8  9  3 56 24 10 14 57 26  3 58 27 59 60 28 13 61]\n",
      "[ 0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25\n",
      " 12  8  9  3 56 24 10 14 57 26  3 58 27 59 60 28 13 61 62]\n",
      "[ 0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12\n",
      "  8  9  3 56 24 10 14 57 26  3 58 27 59 60 28 13 61 62 63]\n",
      "[ 0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8\n",
      "  9  3 56 24 10 14 57 26  3 58 27 59 60 28 13 61 62 63 19]\n",
      "[ 0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9\n",
      "  3 56 24 10 14 57 26  3 58 27 59 60 28 13 61 62 63 19  8]\n",
      "[ 0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3\n",
      " 56 24 10 14 57 26  3 58 27 59 60 28 13 61 62 63 19  8  9]\n",
      "[ 0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56\n",
      " 24 10 14 57 26  3 58 27 59 60 28 13 61 62 63 19  8  9 64]\n",
      "[ 0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24\n",
      " 10 14 57 26  3 58 27 59 60 28 13 61 62 63 19  8  9 64 65]\n",
      "[ 0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10\n",
      " 14 57 26  3 58 27 59 60 28 13 61 62 63 19  8  9 64 65  5]\n",
      "[ 7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10 14\n",
      " 57 26  3 58 27 59 60 28 13 61 62 63 19  8  9 64 65  5  6]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13 20]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13 20 66]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13 20 66 67]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13 20 66 67 68]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 13 20 66 67 68  5]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 13 20 66 67 68  5  6]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 13 20 66 67 68  5  6  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 13 20 66 67 68  5  6  1 69]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 13 20 66 67 68  5  6  1 69 26]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 13 20 66 67 68  5  6  1 69 26 70]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 13 20 66 67 68  5  6  1 69 26 70  4]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 13 20 66 67 68  5  6  1 69 26 70  4 11]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29  2 16]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29  2 16 71]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29  2 16 71 72]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 29  2 16 71 72 15]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 29  2 16 71 72 15 25]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 29  2 16 71 72 15 25 73]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 74  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 74  2 16]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 74  2 16 30]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 74  2 16 30 22]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 74  2 16 30 22 15]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 74  2 16 30 22 15 12]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 75  7]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 75  7 76]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 75  7 76 10]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 75  7 76 10  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 75  7 76 10  2 77]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 75  7 76 10  2 77 78]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 75  7 76 10  2 77 78 79]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 75  7 76 10  2 77 78 79 14]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 75  7 76 10  2 77 78 79 14 80]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 75  7 76 10  2 77 78 79 14 80  7]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 75  7 76 10  2 77 78 79 14 80  7 81]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 75  7 76 10  2 77 78 79 14 80  7 81  5]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 75  7 76 10  2 77 78 79 14 80  7 81  5 82]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0 75  7 76 10  2 77 78 79 14 80  7 81  5 82 83]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0 75  7 76 10  2 77 78 79 14 80  7 81  5 82 83 84]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 85]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 85  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 85  2 16]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 85  2 16 28]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 29 85  2 16 28 86]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 29 85  2 16 28 86 87]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 29 85  2 16 28 86 87  4]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 29 85  2 16 28 86 87  4 88]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 29 85  2 16 28 86 87  4 88  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 29 85  2 16 28 86 87  4 88  1 89]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 29 85  2 16 28 86 87  4 88  1 89  5]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 29 85  2 16 28 86 87  4 88  1 89  5  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 29 85  2 16 28 86 87  4 88  1 89  5  2 27]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 90 91]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 90 91  7]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 90 91  7 92]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 90 91  7 92  3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 90 91  7 92  3 93]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 90 91  7 92  3 93 30]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 90 91  7 92  3 93 30 94]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 90 91  7 92  3 93 30 94  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 90 91  7 92  3 93 30 94  1 95]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 90 91  7 92  3 93 30 94  1 95 96]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 90 91  7 92  3 93 30 94  1 95 96 97]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 90 91  7 92  3 93 30 94  1 95 96 97 98]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 90 91  7 92  3 93 30 94  1 95 96 97 98 99]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0 90 91  7 92  3 93 30 94  1 95 96 97 98 99 23]\n"
     ]
    }
   ],
   "source": [
    "# add padding to the sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "print(\"Max sequence length:\", max_sequence_length)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "# print (input_sequences) in the format of list of lists and \\n after every element\n",
    "for seq in input_sequences:\n",
    "    print(seq, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f611c0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (150, 42)\n",
      "Shape of Y: (150, 100)\n"
     ]
    }
   ],
   "source": [
    "X= input_sequences[:,:-1]  # all rows, all columns except last\n",
    "Y= input_sequences[:,-1]   # all rows, last column\n",
    "\n",
    "# make it a multi-hot encoded vector and mulitclass classification problem\n",
    "# make maxlen classes , if model gives [ 0.1 , 0.6 ,0.3] then it means the next word is the second word in the vocabulary  ie --> [0,1,0] vector\n",
    "\n",
    "Y = tensorflow.keras.utils.to_categorical(Y, num_classes=total_words) # len(word_index) + 1 (+1 kyuyki 0 se index start hoga vector ka)\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc0b769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [embedding] -> [LSTM] -> [Dense] -> [Softmax]\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ab7ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf_numpy2/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025-08-13 21:45:56.818503: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Pro\n",
      "2025-08-13 21:45:56.818522: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2025-08-13 21:45:56.818525: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1755101756.818535  159915 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1755101756.818551  159915 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_words, output_dim=50, input_length=max_sequence_length-1))\n",
    "model.add(LSTM(100)) # 100 nodes in LSTM layer\n",
    "model.add(Dense(total_words, activation='softmax')) # total_words classes\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "711a2a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 21:45:57.326864: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.0133 - loss: 4.6056 \n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0733 - loss: 4.5926\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0800 - loss: 4.5736\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0533 - loss: 4.5050\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0467 - loss: 4.4654\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0467 - loss: 4.3770\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0533 - loss: 4.3409\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0467 - loss: 4.2979\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0800 - loss: 4.2532\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0667 - loss: 4.2105\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0733 - loss: 4.1629\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0733 - loss: 4.1368\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0733 - loss: 4.0773\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0733 - loss: 4.0317   \n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0667 - loss: 3.9811\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0733 - loss: 3.9319\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0800 - loss: 3.8751\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0933 - loss: 3.8291\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0867 - loss: 3.7845\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0867 - loss: 3.7333\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0933 - loss: 3.6836\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0933 - loss: 3.6272\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0933 - loss: 3.5762\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.1000 - loss: 3.5349\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1067 - loss: 3.4875\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1333 - loss: 3.4371\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1200 - loss: 3.3626\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1733 - loss: 3.3347\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1600 - loss: 3.2652\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1667 - loss: 3.2183\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2333 - loss: 3.1318\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2067 - loss: 3.0773\n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2133 - loss: 3.0260\n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.2533 - loss: 2.9668\n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2467 - loss: 2.9045\n",
      "Epoch 36/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.2667 - loss: 2.8529\n",
      "Epoch 37/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2533 - loss: 2.7960\n",
      "Epoch 38/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.2933 - loss: 2.7490\n",
      "Epoch 39/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3200 - loss: 2.6972\n",
      "Epoch 40/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3067 - loss: 2.6439\n",
      "Epoch 41/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3000 - loss: 2.6001\n",
      "Epoch 42/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3400 - loss: 2.5366\n",
      "Epoch 43/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3333 - loss: 2.4961\n",
      "Epoch 44/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3267 - loss: 2.4632\n",
      "Epoch 45/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4000 - loss: 2.4017\n",
      "Epoch 46/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3867 - loss: 2.3615\n",
      "Epoch 47/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4067 - loss: 2.3119\n",
      "Epoch 48/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4267 - loss: 2.2649\n",
      "Epoch 49/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4333 - loss: 2.2210\n",
      "Epoch 50/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4333 - loss: 2.1834\n",
      "Epoch 51/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4733 - loss: 2.1411\n",
      "Epoch 52/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4533 - loss: 2.1098\n",
      "Epoch 53/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5000 - loss: 2.0502\n",
      "Epoch 54/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5333 - loss: 2.0111\n",
      "Epoch 55/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5533 - loss: 1.9713\n",
      "Epoch 56/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5667 - loss: 1.9292\n",
      "Epoch 57/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6000 - loss: 1.8957\n",
      "Epoch 58/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6133 - loss: 1.8569\n",
      "Epoch 59/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6133 - loss: 1.8195\n",
      "Epoch 60/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6133 - loss: 1.7994\n",
      "Epoch 61/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6067 - loss: 1.7564\n",
      "Epoch 62/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6467 - loss: 1.7301\n",
      "Epoch 63/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6333 - loss: 1.6997\n",
      "Epoch 64/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6667 - loss: 1.6597\n",
      "Epoch 65/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6667 - loss: 1.6316\n",
      "Epoch 66/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7000 - loss: 1.5984\n",
      "Epoch 67/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7467 - loss: 1.5667\n",
      "Epoch 68/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7400 - loss: 1.5378\n",
      "Epoch 69/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7400 - loss: 1.5146\n",
      "Epoch 70/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7467 - loss: 1.4891\n",
      "Epoch 71/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7533 - loss: 1.4629\n",
      "Epoch 72/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7800 - loss: 1.4383\n",
      "Epoch 73/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7733 - loss: 1.4102\n",
      "Epoch 74/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8600 - loss: 1.3923\n",
      "Epoch 75/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8667 - loss: 1.3548\n",
      "Epoch 76/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8600 - loss: 1.3398\n",
      "Epoch 77/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8600 - loss: 1.3081\n",
      "Epoch 78/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9067 - loss: 1.2822\n",
      "Epoch 79/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8933 - loss: 1.2572\n",
      "Epoch 80/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8933 - loss: 1.2393\n",
      "Epoch 81/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9000 - loss: 1.2136\n",
      "Epoch 82/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9333 - loss: 1.1937\n",
      "Epoch 83/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9467 - loss: 1.1658\n",
      "Epoch 84/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9400 - loss: 1.1527\n",
      "Epoch 85/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9400 - loss: 1.1341\n",
      "Epoch 86/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9600 - loss: 1.1087\n",
      "Epoch 87/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9667 - loss: 1.0909\n",
      "Epoch 88/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9533 - loss: 1.0775\n",
      "Epoch 89/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9800 - loss: 1.0524\n",
      "Epoch 90/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9800 - loss: 1.0396\n",
      "Epoch 91/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9733 - loss: 1.0325\n",
      "Epoch 92/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9667 - loss: 1.0240\n",
      "Epoch 93/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9533 - loss: 0.9955\n",
      "Epoch 94/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9800 - loss: 0.9716\n",
      "Epoch 95/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9733 - loss: 0.9530\n",
      "Epoch 96/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9733 - loss: 0.9473\n",
      "Epoch 97/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9800 - loss: 0.9140\n",
      "Epoch 98/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9867 - loss: 0.9010\n",
      "Epoch 99/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9867 - loss: 0.8754\n",
      "Epoch 100/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9867 - loss: 0.8597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x32c6301d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b90f9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list: [8, 9, 3, 56, 1]\n",
      "Padded token list: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 56  1]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 511ms/step\n",
      "Predicted: [[4.78403163e-05 5.88564994e-03 1.12459925e-03 8.89099687e-02\n",
      "  1.08715266e-01 2.24183081e-03 1.93078734e-03 8.78494047e-03\n",
      "  2.28061705e-04 9.88772837e-04 1.64237171e-01 1.90362521e-02\n",
      "  1.21858865e-02 5.51327271e-03 8.42294190e-03 3.32802162e-03\n",
      "  9.59391182e-05 8.92395526e-03 5.42643387e-03 1.20971083e-04\n",
      "  7.93297892e-04 2.18548309e-02 2.55613914e-03 3.51257513e-05\n",
      "  1.07379514e-03 6.39069499e-03 4.64438833e-03 2.89152958e-04\n",
      "  1.32296723e-03 1.04030405e-04 4.76632454e-02 2.38379143e-05\n",
      "  1.05576066e-03 5.59936687e-02 9.37405154e-02 1.33956507e-01\n",
      "  7.95163773e-03 8.14115687e-04 1.77534370e-04 1.36119022e-04\n",
      "  2.52930971e-04 1.99535512e-04 1.20023615e-04 8.70185249e-05\n",
      "  8.83410830e-05 4.05590545e-05 7.51758635e-05 1.15098173e-05\n",
      "  2.44973708e-05 1.69232426e-05 6.43593303e-05 5.65241935e-05\n",
      "  4.57380991e-03 4.50414047e-03 6.96314848e-04 6.09603187e-04\n",
      "  6.40368962e-04 5.19607856e-04 2.31010286e-04 8.35131650e-05\n",
      "  8.18171611e-05 9.38088342e-05 2.39193469e-05 2.76159371e-05\n",
      "  6.73731993e-05 4.19979442e-05 1.74405333e-03 2.74160737e-03\n",
      "  5.16995601e-03 4.57551377e-03 2.11440492e-03 7.43160897e-04\n",
      "  1.34750269e-03 1.61275715e-02 8.60329455e-05 2.68839631e-05\n",
      "  9.30280983e-03 2.30683805e-03 4.90417331e-03 6.16222480e-03\n",
      "  1.04533671e-03 7.15964881e-04 1.97543719e-04 2.83606467e-04\n",
      "  2.93893478e-04 3.38528131e-04 2.40256800e-03 7.04117958e-03\n",
      "  4.02056379e-03 2.01435899e-03 7.92998253e-05 2.81191664e-03\n",
      "  1.99197363e-02 5.29373810e-02 4.58617834e-03 2.05430342e-03\n",
      "  1.28571200e-03 2.88548967e-04 2.52428668e-04 1.19949240e-04]]\n",
      "Predicted index: 10\n",
      "Predicted word probability: 0.16423717\n",
      "Predicted word: that\n"
     ]
    }
   ],
   "source": [
    "# lets predict the next word:\n",
    "text = \"This is a temporary fund to\"\n",
    "# tokenize the text\n",
    "token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "print(\"Token list:\", token_list)\n",
    "# pad the token list\n",
    "token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "print(\"Padded token list:\", token_list)\n",
    "# predict the next word\n",
    "predicted = model.predict(token_list)\n",
    "print(\"Predicted:\", predicted) # har word aaney ki probability\n",
    "\n",
    "# get the index of the word with max probability\n",
    "predicted_index = predicted.argmax(axis=-1)[0]\n",
    "print(\"Predicted index:\", predicted_index )\n",
    "print(\"Predicted word probability:\", predicted[0][predicted_index]) # probability of the predicted word\n",
    "print(\"Predicted word:\", tokenizer.index_word[predicted_index]) # get the word from the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94d57bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Predicted word: dashboard\n",
      "Updated text: schedule outline on dashboard\n",
      "Predicted word probability: 0.34158808\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Predicted word: for\n",
      "Updated text: schedule outline on dashboard for\n",
      "Predicted word probability: 0.09901902\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Predicted word: the\n",
      "Updated text: schedule outline on dashboard for the\n",
      "Predicted word probability: 0.07947934\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Predicted word: we\n",
      "Updated text: schedule outline on dashboard for the we\n",
      "Predicted word probability: 0.195532\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Predicted word: we\n",
      "Updated text: schedule outline on dashboard for the we we\n",
      "Predicted word probability: 0.21544288\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Predicted word: have\n",
      "Updated text: schedule outline on dashboard for the we we have\n",
      "Predicted word probability: 0.2094165\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Predicted word: have\n",
      "Updated text: schedule outline on dashboard for the we we have have\n",
      "Predicted word probability: 0.23497881\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Predicted word: in\n",
      "Updated text: schedule outline on dashboard for the we we have have in\n",
      "Predicted word probability: 0.42120725\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Predicted word: for\n",
      "Updated text: schedule outline on dashboard for the we we have have in for\n",
      "Predicted word probability: 0.25972405\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Predicted word: all\n",
      "Updated text: schedule outline on dashboard for the we we have have in for all\n",
      "Predicted word probability: 0.28739095\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"schedule outline on\"\n",
    "# lets try to predict atleast 5 words by giving the predicted word as input\n",
    "for _ in range(10):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "    predicted = model.predict(token_list)\n",
    "    predicted_index = predicted.argmax(axis=-1)[0]\n",
    "    text += \" \" + tokenizer.index_word[predicted_index]\n",
    "    print(\"Predicted word:\", tokenizer.index_word[predicted_index])\n",
    "    print(\"Updated text:\", text)\n",
    "    print(\"Predicted word probability:\", predicted[0][predicted_index]) # probability of the predicted word\n",
    "    print(\"--------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_numpy2)",
   "language": "python",
   "name": "tf_numpy2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
