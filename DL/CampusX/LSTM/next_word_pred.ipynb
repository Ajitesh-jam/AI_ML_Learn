{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4e90387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal\n",
    "#To build a next word prediction model using LSTM in TensorFlow/Keras.\n",
    "\n",
    "# say the line is : \"Hi my name is Ajitesh and I am learning deep learning\"\n",
    "# toh its like \n",
    "# think in terms of making it supervised learning model \n",
    "# like predicting the next word \n",
    "\n",
    "# Input           | Ouput\n",
    "# Hi              | my\n",
    "# Hi my           | name\n",
    "# Hi my name      | is\n",
    "# Hi my name is   | Ajitesh\n",
    "# Hi my name is Ajitesh | and\n",
    "# Hi my name is Ajitesh and | I\n",
    "# Hi my name is Ajitesh and I | am\n",
    "# Hi my name is Ajitesh and I am | learning\n",
    "# Hi my name is Ajitesh and I am learning | deep\n",
    "# Hi my name is Ajitesh and I am learning deep | learning\n",
    "\n",
    "\n",
    "# convert the text into sequences of integers encoded \n",
    "\n",
    "\n",
    "\n",
    "data= \"\"\"Hey Ajitesh, \n",
    "This is a reminder that we've accepted your application to ETHOnline 2025. It would be great if you are able to confirm your attendance by clicking the link below and staking within the next 24 hours.\n",
    "Hacker Dashboard\n",
    "RSVP for ETHOnline 2025 here\n",
    "In order to confirm your attendance, we require you to stake a small deposit on our dashboard. This is a temporary deposit that we return after a successful hackathon participation (more info here). Let us know if this is an issue for you.\n",
    "Here are some important things for you to do after confirming your attendance:\n",
    "Check the event schedule outline on our website\n",
    "Join the event Discord (link on dashboard)!\n",
    "Keep in mind that the Code of Conduct we have in place for all ETHGlobal events\n",
    "Check out the Event Info Center → your go-to resource for the hackathon\n",
    "Say hello in #find-a-team discord channel to chat with other hackers (post staking)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89044224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey Ajitesh, \\nThis is a reminder that we've accepted your application to ETHOnline 2025. It would be great if you are able to confirm your attendance by clicking the link below and staking within the next 24 hours.\\nHacker Dashboard\\nRSVP for ETHOnline 2025 here\\nIn order to confirm your attendance, we require you to stake a small deposit on our dashboard. This is a temporary deposit that we return after a successful hackathon participation (more info here). Let us know if this is an issue for you.\\nHere are some important things for you to do after confirming your attendance:\\nCheck the event schedule outline on our website\\nJoin the event Discord (link on dashboard)!\\nKeep in mind that the Code of Conduct we have in place for all ETHGlobal events\\nCheck out the Event Info Center → your go-to resource for the hackathon\\nSay hello in #find-a-team discord channel to chat with other hackers (post staking)\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3df63ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c524b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 1, 'the': 2, 'a': 3, 'your': 4, 'for': 5, 'you': 6, 'in': 7, 'this': 8, 'is': 9, 'that': 10, 'attendance': 11, 'dashboard': 12, 'here': 13, 'we': 14, 'on': 15, 'event': 16, 'ethonline': 17, '2025': 18, 'if': 19, 'are': 20, 'confirm': 21, 'link': 22, 'staking': 23, 'deposit': 24, 'our': 25, 'after': 26, 'hackathon': 27, 'info': 28, 'check': 29, 'discord': 30, 'hey': 31, 'ajitesh': 32, 'reminder': 33, \"we've\": 34, 'accepted': 35, 'application': 36, 'it': 37, 'would': 38, 'be': 39, 'great': 40, 'able': 41, 'by': 42, 'clicking': 43, 'below': 44, 'and': 45, 'within': 46, 'next': 47, '24': 48, 'hours': 49, 'hacker': 50, 'rsvp': 51, 'order': 52, 'require': 53, 'stake': 54, 'small': 55, 'temporary': 56, 'return': 57, 'successful': 58, 'participation': 59, 'more': 60, 'let': 61, 'us': 62, 'know': 63, 'an': 64, 'issue': 65, 'some': 66, 'important': 67, 'things': 68, 'do': 69, 'confirming': 70, 'schedule': 71, 'outline': 72, 'website': 73, 'join': 74, 'keep': 75, 'mind': 76, 'code': 77, 'of': 78, 'conduct': 79, 'have': 80, 'place': 81, 'all': 82, 'ethglobal': 83, 'events': 84, 'out': 85, 'center': 86, '→': 87, 'go': 88, 'resource': 89, 'say': 90, 'hello': 91, 'find': 92, 'team': 93, 'channel': 94, 'chat': 95, 'with': 96, 'other': 97, 'hackers': 98, 'post': 99}\n",
      "[31, 32]\n",
      "[8, 9]\n",
      "[8, 9, 3]\n",
      "[8, 9, 3, 33]\n",
      "[8, 9, 3, 33, 10]\n",
      "[8, 9, 3, 33, 10, 34]\n",
      "[8, 9, 3, 33, 10, 34, 35]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45, 23]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45, 23, 46]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45, 23, 46, 2]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45, 23, 46, 2, 47]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45, 23, 46, 2, 47, 48]\n",
      "[8, 9, 3, 33, 10, 34, 35, 4, 36, 1, 17, 18, 37, 38, 39, 40, 19, 6, 20, 41, 1, 21, 4, 11, 42, 43, 2, 22, 44, 45, 23, 46, 2, 47, 48, 49]\n",
      "[50, 12]\n",
      "[51, 5]\n",
      "[51, 5, 17]\n",
      "[51, 5, 17, 18]\n",
      "[51, 5, 17, 18, 13]\n",
      "[7, 52]\n",
      "[7, 52, 1]\n",
      "[7, 52, 1, 21]\n",
      "[7, 52, 1, 21, 4]\n",
      "[7, 52, 1, 21, 4, 11]\n",
      "[7, 52, 1, 21, 4, 11, 14]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19, 8]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19, 8, 9]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19, 8, 9, 64]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19, 8, 9, 64, 65]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19, 8, 9, 64, 65, 5]\n",
      "[7, 52, 1, 21, 4, 11, 14, 53, 6, 1, 54, 3, 55, 24, 15, 25, 12, 8, 9, 3, 56, 24, 10, 14, 57, 26, 3, 58, 27, 59, 60, 28, 13, 61, 62, 63, 19, 8, 9, 64, 65, 5, 6]\n",
      "[13, 20]\n",
      "[13, 20, 66]\n",
      "[13, 20, 66, 67]\n",
      "[13, 20, 66, 67, 68]\n",
      "[13, 20, 66, 67, 68, 5]\n",
      "[13, 20, 66, 67, 68, 5, 6]\n",
      "[13, 20, 66, 67, 68, 5, 6, 1]\n",
      "[13, 20, 66, 67, 68, 5, 6, 1, 69]\n",
      "[13, 20, 66, 67, 68, 5, 6, 1, 69, 26]\n",
      "[13, 20, 66, 67, 68, 5, 6, 1, 69, 26, 70]\n",
      "[13, 20, 66, 67, 68, 5, 6, 1, 69, 26, 70, 4]\n",
      "[13, 20, 66, 67, 68, 5, 6, 1, 69, 26, 70, 4, 11]\n",
      "[29, 2]\n",
      "[29, 2, 16]\n",
      "[29, 2, 16, 71]\n",
      "[29, 2, 16, 71, 72]\n",
      "[29, 2, 16, 71, 72, 15]\n",
      "[29, 2, 16, 71, 72, 15, 25]\n",
      "[29, 2, 16, 71, 72, 15, 25, 73]\n",
      "[74, 2]\n",
      "[74, 2, 16]\n",
      "[74, 2, 16, 30]\n",
      "[74, 2, 16, 30, 22]\n",
      "[74, 2, 16, 30, 22, 15]\n",
      "[74, 2, 16, 30, 22, 15, 12]\n",
      "[75, 7]\n",
      "[75, 7, 76]\n",
      "[75, 7, 76, 10]\n",
      "[75, 7, 76, 10, 2]\n",
      "[75, 7, 76, 10, 2, 77]\n",
      "[75, 7, 76, 10, 2, 77, 78]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80, 7]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80, 7, 81]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80, 7, 81, 5]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80, 7, 81, 5, 82]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80, 7, 81, 5, 82, 83]\n",
      "[75, 7, 76, 10, 2, 77, 78, 79, 14, 80, 7, 81, 5, 82, 83, 84]\n",
      "[29, 85]\n",
      "[29, 85, 2]\n",
      "[29, 85, 2, 16]\n",
      "[29, 85, 2, 16, 28]\n",
      "[29, 85, 2, 16, 28, 86]\n",
      "[29, 85, 2, 16, 28, 86, 87]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4, 88]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4, 88, 1]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4, 88, 1, 89]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4, 88, 1, 89, 5]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4, 88, 1, 89, 5, 2]\n",
      "[29, 85, 2, 16, 28, 86, 87, 4, 88, 1, 89, 5, 2, 27]\n",
      "[90, 91]\n",
      "[90, 91, 7]\n",
      "[90, 91, 7, 92]\n",
      "[90, 91, 7, 92, 3]\n",
      "[90, 91, 7, 92, 3, 93]\n",
      "[90, 91, 7, 92, 3, 93, 30]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1, 95]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1, 95, 96]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1, 95, 96, 97]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1, 95, 96, 97, 98]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1, 95, 96, 97, 98, 99]\n",
      "[90, 91, 7, 92, 3, 93, 30, 94, 1, 95, 96, 97, 98, 99, 23]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "#print encoded text\n",
    "input_sequences = []\n",
    "for line in data.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# print (input_sequences) in the format of list of lists and \\n after every element\n",
    "for seq in input_sequences:\n",
    "    print(seq, end='\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06fb77cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 43\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 31 32]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 8 9]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 8 9 3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40 19]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40 19  6]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40 19  6 20]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8\n",
      "  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40 19  6 20 41]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9\n",
      "  3 33 10 34 35  4 36  1 17 18 37 38 39 40 19  6 20 41  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3\n",
      " 33 10 34 35  4 36  1 17 18 37 38 39 40 19  6 20 41  1 21]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33\n",
      " 10 34 35  4 36  1 17 18 37 38 39 40 19  6 20 41  1 21  4]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10\n",
      " 34 35  4 36  1 17 18 37 38 39 40 19  6 20 41  1 21  4 11]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34\n",
      " 35  4 36  1 17 18 37 38 39 40 19  6 20 41  1 21  4 11 42]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35\n",
      "  4 36  1 17 18 37 38 39 40 19  6 20 41  1 21  4 11 42 43]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4\n",
      " 36  1 17 18 37 38 39 40 19  6 20 41  1 21  4 11 42 43  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36\n",
      "  1 17 18 37 38 39 40 19  6 20 41  1 21  4 11 42 43  2 22]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1\n",
      " 17 18 37 38 39 40 19  6 20 41  1 21  4 11 42 43  2 22 44]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17\n",
      " 18 37 38 39 40 19  6 20 41  1 21  4 11 42 43  2 22 44 45]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18\n",
      " 37 38 39 40 19  6 20 41  1 21  4 11 42 43  2 22 44 45 23]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37\n",
      " 38 39 40 19  6 20 41  1 21  4 11 42 43  2 22 44 45 23 46]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38\n",
      " 39 40 19  6 20 41  1 21  4 11 42 43  2 22 44 45 23 46  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39\n",
      " 40 19  6 20 41  1 21  4 11 42 43  2 22 44 45 23 46  2 47]\n",
      "[ 0  0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40\n",
      " 19  6 20 41  1 21  4 11 42 43  2 22 44 45 23 46  2 47 48]\n",
      "[ 0  0  0  0  0  0  0  8  9  3 33 10 34 35  4 36  1 17 18 37 38 39 40 19\n",
      "  6 20 41  1 21  4 11 42 43  2 22 44 45 23 46  2 47 48 49]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 50 12]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 51  5]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 51  5 17]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 51  5 17 18]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 51  5 17 18 13]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7\n",
      " 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52\n",
      "  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1\n",
      " 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21\n",
      "  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4\n",
      " 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10 14]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11\n",
      " 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10 14 57]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14\n",
      " 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10 14 57 26]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53\n",
      "  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10 14 57 26  3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6\n",
      "  1 54  3 55 24 15 25 12  8  9  3 56 24 10 14 57 26  3 58]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1\n",
      " 54  3 55 24 15 25 12  8  9  3 56 24 10 14 57 26  3 58 27]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54\n",
      "  3 55 24 15 25 12  8  9  3 56 24 10 14 57 26  3 58 27 59]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3\n",
      " 55 24 15 25 12  8  9  3 56 24 10 14 57 26  3 58 27 59 60]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55\n",
      " 24 15 25 12  8  9  3 56 24 10 14 57 26  3 58 27 59 60 28]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24\n",
      " 15 25 12  8  9  3 56 24 10 14 57 26  3 58 27 59 60 28 13]\n",
      "[ 0  0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15\n",
      " 25 12  8  9  3 56 24 10 14 57 26  3 58 27 59 60 28 13 61]\n",
      "[ 0  0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25\n",
      " 12  8  9  3 56 24 10 14 57 26  3 58 27 59 60 28 13 61 62]\n",
      "[ 0  0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12\n",
      "  8  9  3 56 24 10 14 57 26  3 58 27 59 60 28 13 61 62 63]\n",
      "[ 0  0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8\n",
      "  9  3 56 24 10 14 57 26  3 58 27 59 60 28 13 61 62 63 19]\n",
      "[ 0  0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9\n",
      "  3 56 24 10 14 57 26  3 58 27 59 60 28 13 61 62 63 19  8]\n",
      "[ 0  0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3\n",
      " 56 24 10 14 57 26  3 58 27 59 60 28 13 61 62 63 19  8  9]\n",
      "[ 0  0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56\n",
      " 24 10 14 57 26  3 58 27 59 60 28 13 61 62 63 19  8  9 64]\n",
      "[ 0  0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24\n",
      " 10 14 57 26  3 58 27 59 60 28 13 61 62 63 19  8  9 64 65]\n",
      "[ 0  7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10\n",
      " 14 57 26  3 58 27 59 60 28 13 61 62 63 19  8  9 64 65  5]\n",
      "[ 7 52  1 21  4 11 14 53  6  1 54  3 55 24 15 25 12  8  9  3 56 24 10 14\n",
      " 57 26  3 58 27 59 60 28 13 61 62 63 19  8  9 64 65  5  6]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13 20]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13 20 66]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13 20 66 67]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13 20 66 67 68]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 13 20 66 67 68  5]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 13 20 66 67 68  5  6]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 13 20 66 67 68  5  6  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 13 20 66 67 68  5  6  1 69]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 13 20 66 67 68  5  6  1 69 26]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 13 20 66 67 68  5  6  1 69 26 70]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 13 20 66 67 68  5  6  1 69 26 70  4]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 13 20 66 67 68  5  6  1 69 26 70  4 11]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29  2 16]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29  2 16 71]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29  2 16 71 72]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 29  2 16 71 72 15]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 29  2 16 71 72 15 25]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 29  2 16 71 72 15 25 73]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 74  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 74  2 16]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 74  2 16 30]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 74  2 16 30 22]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 74  2 16 30 22 15]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 74  2 16 30 22 15 12]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 75  7]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 75  7 76]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 75  7 76 10]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 75  7 76 10  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 75  7 76 10  2 77]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 75  7 76 10  2 77 78]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 75  7 76 10  2 77 78 79]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 75  7 76 10  2 77 78 79 14]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 75  7 76 10  2 77 78 79 14 80]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 75  7 76 10  2 77 78 79 14 80  7]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 75  7 76 10  2 77 78 79 14 80  7 81]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 75  7 76 10  2 77 78 79 14 80  7 81  5]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 75  7 76 10  2 77 78 79 14 80  7 81  5 82]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0 75  7 76 10  2 77 78 79 14 80  7 81  5 82 83]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0 75  7 76 10  2 77 78 79 14 80  7 81  5 82 83 84]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 85]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 85  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 85  2 16]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 85  2 16 28]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 29 85  2 16 28 86]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 29 85  2 16 28 86 87]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 29 85  2 16 28 86 87  4]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 29 85  2 16 28 86 87  4 88]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 29 85  2 16 28 86 87  4 88  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 29 85  2 16 28 86 87  4 88  1 89]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 29 85  2 16 28 86 87  4 88  1 89  5]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 29 85  2 16 28 86 87  4 88  1 89  5  2]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 29 85  2 16 28 86 87  4 88  1 89  5  2 27]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 90 91]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 90 91  7]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 90 91  7 92]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 90 91  7 92  3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0 90 91  7 92  3 93]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 90 91  7 92  3 93 30]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 90 91  7 92  3 93 30 94]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 90 91  7 92  3 93 30 94  1]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 90 91  7 92  3 93 30 94  1 95]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 90 91  7 92  3 93 30 94  1 95 96]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 90 91  7 92  3 93 30 94  1 95 96 97]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 90 91  7 92  3 93 30 94  1 95 96 97 98]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 90 91  7 92  3 93 30 94  1 95 96 97 98 99]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0 90 91  7 92  3 93 30 94  1 95 96 97 98 99 23]\n"
     ]
    }
   ],
   "source": [
    "# add padding to the sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "print(\"Max sequence length:\", max_sequence_length)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "# print (input_sequences) in the format of list of lists and \\n after every element\n",
    "for seq in input_sequences:\n",
    "    print(seq, end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f611c0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (150, 42)\n",
      "Shape of Y: (150, 100)\n"
     ]
    }
   ],
   "source": [
    "X= input_sequences[:,:-1]  # all rows, all columns except last\n",
    "Y= input_sequences[:,-1]   # all rows, last column\n",
    "\n",
    "# make it a multi-hot encoded vector and mulitclass classification problem\n",
    "# make maxlen classes , if model gives [ 0.1 , 0.6 ,0.3] then it means the next word is the second word in the vocabulary  ie --> [0,1,0] vector\n",
    "\n",
    "Y = tensorflow.keras.utils.to_categorical(Y, num_classes=total_words) # len(word_index) + 1 (+1 kyuyki 0 se index start hoga vector ka)\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc0b769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [embedding] -> [LSTM] -> [Dense] -> [Softmax]\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ab7ade3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_words, output_dim=50, input_length=max_sequence_length-1))\n",
    "model.add(LSTM(100)) # 100 nodes in LSTM layer\n",
    "model.add(Dense(total_words, activation='softmax')) # total_words classes\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "711a2a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.0067 - loss: 4.6055    \n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0800 - loss: 4.5921\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0800 - loss: 4.5748\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0467 - loss: 4.4974\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0467 - loss: 4.3994   \n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0467 - loss: 4.3634\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0667 - loss: 4.3219\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0667 - loss: 4.2902\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0667 - loss: 4.2413\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0733 - loss: 4.2065\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0667 - loss: 4.1689\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0733 - loss: 4.1256\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0733 - loss: 4.0768\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0667 - loss: 4.0291\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0667 - loss: 3.9911\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0933 - loss: 3.9453\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0800 - loss: 3.9037\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0800 - loss: 3.8718\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0800 - loss: 3.8193\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0933 - loss: 3.7501\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1000 - loss: 3.7140\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0933 - loss: 3.6584\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0933 - loss: 3.6191\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1000 - loss: 3.5666\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.1000 - loss: 3.5277\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1200 - loss: 3.4697\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1200 - loss: 3.4188\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.1333 - loss: 3.3522\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.1267 - loss: 3.3006\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.1600 - loss: 3.2439\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.1867 - loss: 3.1819\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.2133 - loss: 3.1304\n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2333 - loss: 3.0711\n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2267 - loss: 3.0205\n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.2000 - loss: 2.9708\n",
      "Epoch 36/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2000 - loss: 2.9030\n",
      "Epoch 37/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2733 - loss: 2.8604\n",
      "Epoch 38/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2533 - loss: 2.8233\n",
      "Epoch 39/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3000 - loss: 2.7626\n",
      "Epoch 40/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3000 - loss: 2.7113\n",
      "Epoch 41/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3000 - loss: 2.6732\n",
      "Epoch 42/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3333 - loss: 2.6133\n",
      "Epoch 43/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2933 - loss: 2.5705\n",
      "Epoch 44/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3533 - loss: 2.5214\n",
      "Epoch 45/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3600 - loss: 2.4804\n",
      "Epoch 46/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3867 - loss: 2.4286\n",
      "Epoch 47/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3533 - loss: 2.3824\n",
      "Epoch 48/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3400 - loss: 2.3536\n",
      "Epoch 49/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4000 - loss: 2.2943\n",
      "Epoch 50/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4600 - loss: 2.2539\n",
      "Epoch 51/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4733 - loss: 2.2070\n",
      "Epoch 52/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4933 - loss: 2.1652\n",
      "Epoch 53/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5333 - loss: 2.1253\n",
      "Epoch 54/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5267 - loss: 2.0893\n",
      "Epoch 55/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5267 - loss: 2.0514\n",
      "Epoch 56/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5200 - loss: 2.0182\n",
      "Epoch 57/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5333 - loss: 1.9866\n",
      "Epoch 58/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5800 - loss: 1.9494\n",
      "Epoch 59/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5933 - loss: 1.9143\n",
      "Epoch 60/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6067 - loss: 1.8840\n",
      "Epoch 61/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6333 - loss: 1.8415\n",
      "Epoch 62/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6467 - loss: 1.8148\n",
      "Epoch 63/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6600 - loss: 1.7725\n",
      "Epoch 64/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6800 - loss: 1.7421\n",
      "Epoch 65/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6600 - loss: 1.7110\n",
      "Epoch 66/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6867 - loss: 1.6843\n",
      "Epoch 67/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7000 - loss: 1.6483\n",
      "Epoch 68/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7333 - loss: 1.6211\n",
      "Epoch 69/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7467 - loss: 1.5918\n",
      "Epoch 70/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7267 - loss: 1.5590\n",
      "Epoch 71/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7733 - loss: 1.5320\n",
      "Epoch 72/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7667 - loss: 1.5164\n",
      "Epoch 73/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7667 - loss: 1.5067\n",
      "Epoch 74/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8000 - loss: 1.4700\n",
      "Epoch 75/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8133 - loss: 1.4484\n",
      "Epoch 76/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8200 - loss: 1.4237\n",
      "Epoch 77/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8467 - loss: 1.3930\n",
      "Epoch 78/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8600 - loss: 1.3609\n",
      "Epoch 79/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8733 - loss: 1.3386\n",
      "Epoch 80/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9133 - loss: 1.3065\n",
      "Epoch 81/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8933 - loss: 1.2887\n",
      "Epoch 82/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9133 - loss: 1.2609\n",
      "Epoch 83/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9200 - loss: 1.2365\n",
      "Epoch 84/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9333 - loss: 1.2195\n",
      "Epoch 85/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8733 - loss: 1.2128\n",
      "Epoch 86/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8800 - loss: 1.2054\n",
      "Epoch 87/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8400 - loss: 1.2122\n",
      "Epoch 88/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8600 - loss: 1.2011\n",
      "Epoch 89/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8933 - loss: 1.1647\n",
      "Epoch 90/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9067 - loss: 1.1314\n",
      "Epoch 91/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8733 - loss: 1.1549\n",
      "Epoch 92/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8800 - loss: 1.1421\n",
      "Epoch 93/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8400 - loss: 1.1548\n",
      "Epoch 94/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9467 - loss: 1.0702\n",
      "Epoch 95/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9467 - loss: 1.0527\n",
      "Epoch 96/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9200 - loss: 1.0356\n",
      "Epoch 97/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9467 - loss: 0.9964\n",
      "Epoch 98/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9667 - loss: 0.9719\n",
      "Epoch 99/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9667 - loss: 0.9530\n",
      "Epoch 100/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9667 - loss: 0.9278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x177551b50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b90f9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list: [8, 9, 3, 56, 1]\n",
      "Padded token list: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  8  9  3 56  1]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Predicted: [[1.5664144e-05 1.1336912e-02 2.5573527e-03 7.7697672e-03 5.2440092e-02\n",
      "  6.9240974e-03 1.2502267e-02 4.2230361e-03 4.3089077e-04 6.6611642e-04\n",
      "  1.1304470e-01 1.8704204e-03 3.0890822e-03 1.1111421e-02 2.6717838e-02\n",
      "  1.6101837e-02 9.3666901e-04 2.1839008e-02 1.6107848e-02 1.7733169e-04\n",
      "  8.6778379e-04 9.5665930e-03 4.1100304e-03 4.5078399e-04 7.5995037e-04\n",
      "  7.5236396e-03 1.8556248e-03 5.2857777e-04 2.5024980e-03 1.6417887e-05\n",
      "  7.2957039e-02 9.6377713e-05 3.0201551e-04 3.3167504e-02 2.3563652e-01\n",
      "  1.0821282e-01 4.2846496e-03 2.5700996e-04 2.1520891e-04 1.9126399e-04\n",
      "  4.2593409e-05 6.0838863e-04 1.7978113e-04 9.9571116e-05 1.2435528e-04\n",
      "  4.2904267e-04 2.1207507e-04 8.1647529e-05 1.0801315e-04 5.2630912e-05\n",
      "  5.2378848e-05 4.1381529e-05 5.4028881e-04 7.8962147e-03 1.8899684e-03\n",
      "  1.2397352e-03 2.1024296e-04 3.7662862e-04 1.8967323e-04 1.9844426e-04\n",
      "  1.1997221e-04 2.9563867e-05 2.8966431e-05 6.8343375e-05 6.4809679e-04\n",
      "  2.4110774e-04 7.0403856e-03 8.8462029e-03 1.2788932e-02 3.1716053e-03\n",
      "  5.6839705e-04 3.7054711e-03 4.7280868e-03 8.1980070e-03 5.9965314e-05\n",
      "  2.0671703e-05 1.3837739e-03 2.4660598e-02 2.6119610e-02 1.4831584e-02\n",
      "  5.7519809e-03 2.0184815e-03 1.1600886e-03 3.3158986e-04 2.1725736e-04\n",
      "  5.4054626e-04 5.8274977e-03 5.4023187e-03 3.5373515e-03 8.6423621e-04\n",
      "  4.1184874e-05 5.0123798e-04 4.5478833e-03 2.0784935e-02 1.3644782e-02\n",
      "  4.9645924e-03 1.3629196e-03 1.8370713e-03 9.9358207e-04 4.7347927e-04]]\n",
      "Predicted index: 34\n",
      "Predicted word probability: 0.23563652\n",
      "Predicted word: we've\n"
     ]
    }
   ],
   "source": [
    "# lets predict the next word:\n",
    "text = \"This is a temporary fund to\"\n",
    "# tokenize the text\n",
    "token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "print(\"Token list:\", token_list)\n",
    "# pad the token list\n",
    "token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "print(\"Padded token list:\", token_list)\n",
    "# predict the next word\n",
    "predicted = model.predict(token_list)\n",
    "print(\"Predicted:\", predicted) # har word aaney ki probability\n",
    "\n",
    "# get the index of the word with max probability\n",
    "predicted_index = predicted.argmax(axis=-1)[0]\n",
    "print(\"Predicted index:\", predicted_index )\n",
    "print(\"Predicted word probability:\", predicted[0][predicted_index]) # probability of the predicted word\n",
    "print(\"Predicted word:\", tokenizer.index_word[predicted_index]) # get the word from the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94d57bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Predicted word: dashboard\n",
      "Updated text: schedule outline on dashboard\n",
      "Predicted word probability: 0.36761096\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Predicted word: our\n",
      "Updated text: schedule outline on dashboard our\n",
      "Predicted word probability: 0.13513485\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Predicted word: our\n",
      "Updated text: schedule outline on dashboard our our\n",
      "Predicted word probability: 0.17272598\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Predicted word: website\n",
      "Updated text: schedule outline on dashboard our our website\n",
      "Predicted word probability: 0.23889928\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Predicted word: website\n",
      "Updated text: schedule outline on dashboard our our website website\n",
      "Predicted word probability: 0.34302557\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Predicted word: website\n",
      "Updated text: schedule outline on dashboard our our website website website\n",
      "Predicted word probability: 0.28325722\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Predicted word: website\n",
      "Updated text: schedule outline on dashboard our our website website website website\n",
      "Predicted word probability: 0.16292317\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Predicted word: for\n",
      "Updated text: schedule outline on dashboard our our website website website website for\n",
      "Predicted word probability: 0.19020072\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Predicted word: for\n",
      "Updated text: schedule outline on dashboard our our website website website website for for\n",
      "Predicted word probability: 0.23254327\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Predicted word: for\n",
      "Updated text: schedule outline on dashboard our our website website website website for for for\n",
      "Predicted word probability: 0.14107372\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"schedule outline on\"\n",
    "# lets try to predict atleast 5 words by giving the predicted word as input\n",
    "for _ in range(10):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "    predicted = model.predict(token_list)\n",
    "    predicted_index = predicted.argmax(axis=-1)[0]\n",
    "    text += \" \" + tokenizer.index_word[predicted_index]\n",
    "    print(\"Predicted word:\", tokenizer.index_word[predicted_index])\n",
    "    print(\"Updated text:\", text)\n",
    "    print(\"Predicted word probability:\", predicted[0][predicted_index]) # probability of the predicted word\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989e5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_numpy2)",
   "language": "python",
   "name": "tf_numpy2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
