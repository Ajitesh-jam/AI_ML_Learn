{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e33491ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow devices:\", tf.config.list_physical_devices())\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a574af90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/ajitesh/.cache/kagglehub/datasets/gpiosenka/cards-image-datasetclassification/versions/2\n"
     ]
    }
   ],
   "source": [
    "# %pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"gpiosenka/cards-image-datasetclassification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28bda3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow numpy pandas matplotlib seaborn scikit-learn\n",
    "from tensorflow import keras \n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "099cbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import load_img\n",
    "import glob\n",
    "\n",
    "# Get all jpg image paths in the dataset directory (including subfolders)\n",
    "image_paths = glob.glob(os.path.join(path, '**', '*.jpg'), recursive=True)\n",
    "\n",
    "# Load images into a list\n",
    "# train_data = [load_img(img_path) for img_path in image_paths]\n",
    "train_data = []\n",
    "labels = []\n",
    "for i in image_paths:\n",
    "    train_data.append(load_img(i))\n",
    "    labels.append(i.split('/')[-2])  # Assuming the label is the folder name\n",
    "\n",
    "train_data[1].size\n",
    "# Convert to DataFrame\n",
    "train_df = pd.DataFrame({\n",
    "    'image': train_data,\n",
    "    'label': labels\n",
    "})\n",
    "train_df.head()\n",
    "# convert jpg images to tensors\n",
    "from keras.preprocessing.image import img_to_array\n",
    "train_df['image'] = train_df['image'].apply(lambda x: img_to_array(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f621c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Convert labels to categorical format\n",
    "coded_labels={\n",
    "\"ace of clubs\" :1       ,    \"eight of hearts\" :2  ,     \"four of clubs\" :3  ,          \"jack of hearts\" :4   ,       \"king of spades\" :5  ,        \"queen of diamonds\" :6       , \"seven of spades\" :7   , \"ten of diamonds\" :8         , \"three of spades\" :9,\n",
    "\"ace of diamonds\" :10     ,   \"eight of spades\" :11     ,    \"four of diamonds\" :12 ,       \"jack of spades\" :13   ,       \"nine of clubs\" :14    ,       \"queen of hearts\" :15         , \"six of clubs\" :16            , \"ten of hearts\" :17           , \"two of clubs\" :18,\n",
    "\"ace of hearts\" :19       ,   \"five of clubs\" :20       ,    \"four of hearts\" :21 ,         \"joker\" :22           ,        \"nine of diamonds\" :23 ,       \"queen of spades\" :24         , \"six of diamonds\" :25         , \"ten of spades\" :26           , \"two of diamonds\" :27,\n",
    "\"ace of spades\" :28       ,   \"five of diamonds\" :29    ,    \"four of spades\" :30 ,         \"king of clubs\" :31   ,        \"nine of hearts\" :32   ,       \"seven of clubs\" :33          , \"six of hearts\" :34           , \"three of clubs\" :35          , \"two of hearts\" :36,\n",
    "\"eight of clubs\" :37      ,   \"five of hearts\" :38      ,    \"jack of clubs\" :39 ,          \"king of diamonds\" :40   ,     \"nine of spades\" :41   ,      \"seven of diamonds\" :42       , \"six of spades\" :43           , \"three of diamonds\" :44       , \"two of spades\" :45,\n",
    "\"eight of diamonds\" :46   ,   \"five of spades\" :47      ,    \"jack of diamonds\" :48 ,       \"king of hearts\" :49   ,       \"queen of clubs\" :50   ,      \"seven of hearts\" :51        , \"ten of clubs\" :52            , \"three of hearts\" :53\n",
    "}\n",
    "def label_to_one_hot(label):\n",
    "    one_hot = np.zeros(53)\n",
    "    one_hot[coded_labels[label]-1] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train = np.array([label_to_one_hot(label) for label in train_df['label']])\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ea2ffb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[236., 221., 214.],\n",
       "         [235., 220., 213.],\n",
       "         [219., 204., 199.],\n",
       "         ...,\n",
       "         [220., 203., 195.],\n",
       "         [220., 203., 193.],\n",
       "         [221., 205., 192.]],\n",
       "\n",
       "        [[242., 227., 220.],\n",
       "         [240., 225., 218.],\n",
       "         [218., 203., 196.],\n",
       "         ...,\n",
       "         [219., 202., 194.],\n",
       "         [220., 203., 193.],\n",
       "         [221., 205., 192.]],\n",
       "\n",
       "        [[236., 221., 214.],\n",
       "         [239., 224., 217.],\n",
       "         [218., 203., 196.],\n",
       "         ...,\n",
       "         [220., 201., 194.],\n",
       "         [221., 203., 193.],\n",
       "         [222., 204., 194.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[186., 175., 173.],\n",
       "         [190., 179., 173.],\n",
       "         [199., 186., 177.],\n",
       "         ...,\n",
       "         [228., 209., 177.],\n",
       "         [226., 210., 177.],\n",
       "         [225., 209., 176.]],\n",
       "\n",
       "        [[187., 176., 180.],\n",
       "         [185., 175., 176.],\n",
       "         [187., 176., 170.],\n",
       "         ...,\n",
       "         [228., 208., 175.],\n",
       "         [231., 207., 173.],\n",
       "         [231., 205., 170.]],\n",
       "\n",
       "        [[190., 180., 188.],\n",
       "         [181., 172., 175.],\n",
       "         [176., 166., 164.],\n",
       "         ...,\n",
       "         [229., 206., 174.],\n",
       "         [232., 203., 169.],\n",
       "         [234., 201., 166.]]],\n",
       "\n",
       "\n",
       "       [[[244., 200., 223.],\n",
       "         [228., 186., 206.],\n",
       "         [234., 198., 212.],\n",
       "         ...,\n",
       "         [212., 186., 211.],\n",
       "         [228., 202., 229.],\n",
       "         [220., 194., 221.]],\n",
       "\n",
       "        [[224., 182., 204.],\n",
       "         [219., 180., 199.],\n",
       "         [226., 189., 206.],\n",
       "         ...,\n",
       "         [215., 189., 214.],\n",
       "         [222., 196., 223.],\n",
       "         [216., 190., 217.]],\n",
       "\n",
       "        [[220., 183., 201.],\n",
       "         [225., 188., 206.],\n",
       "         [226., 187., 206.],\n",
       "         ...,\n",
       "         [216., 190., 217.],\n",
       "         [218., 192., 219.],\n",
       "         [216., 190., 217.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[201., 191., 226.],\n",
       "         [209., 199., 234.],\n",
       "         [201., 191., 226.],\n",
       "         ...,\n",
       "         [189., 196., 248.],\n",
       "         [191., 200., 243.],\n",
       "         [195., 204., 245.]],\n",
       "\n",
       "        [[195., 185., 220.],\n",
       "         [208., 198., 233.],\n",
       "         [200., 190., 225.],\n",
       "         ...,\n",
       "         [194., 202., 249.],\n",
       "         [201., 205., 250.],\n",
       "         [194., 197., 238.]],\n",
       "\n",
       "        [[202., 192., 227.],\n",
       "         [207., 197., 232.],\n",
       "         [201., 191., 226.],\n",
       "         ...,\n",
       "         [195., 206., 252.],\n",
       "         [194., 197., 240.],\n",
       "         [205., 207., 248.]]],\n",
       "\n",
       "\n",
       "       [[[ 14.,   9.,  15.],\n",
       "         [ 18.,  13.,  19.],\n",
       "         [ 16.,  14.,  19.],\n",
       "         ...,\n",
       "         [ 11.,  10.,  15.],\n",
       "         [ 20.,  19.,  24.],\n",
       "         [253., 252., 255.]],\n",
       "\n",
       "        [[ 16.,  11.,  17.],\n",
       "         [ 12.,  10.,  15.],\n",
       "         [ 15.,  13.,  18.],\n",
       "         ...,\n",
       "         [ 12.,  11.,  16.],\n",
       "         [ 15.,  14.,  19.],\n",
       "         [ 14.,  13.,  18.]],\n",
       "\n",
       "        [[  9.,   7.,  10.],\n",
       "         [ 13.,  11.,  14.],\n",
       "         [ 10.,   8.,  13.],\n",
       "         ...,\n",
       "         [ 28.,  27.,  32.],\n",
       "         [ 16.,  15.,  20.],\n",
       "         [  8.,   7.,  12.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 17.,  18.,  23.],\n",
       "         [ 16.,  17.,  22.],\n",
       "         [ 19.,  20.,  25.],\n",
       "         ...,\n",
       "         [ 41.,  40.,  45.],\n",
       "         [ 42.,  41.,  46.],\n",
       "         [ 19.,  18.,  23.]],\n",
       "\n",
       "        [[ 20.,  21.,  26.],\n",
       "         [ 35.,  36.,  41.],\n",
       "         [ 13.,  14.,  19.],\n",
       "         ...,\n",
       "         [ 30.,  31.,  35.],\n",
       "         [ 93.,  92.,  97.],\n",
       "         [ 15.,  14.,  19.]],\n",
       "\n",
       "        [[ 16.,  17.,  22.],\n",
       "         [ 20.,  21.,  26.],\n",
       "         [ 19.,  20.,  25.],\n",
       "         ...,\n",
       "         [ 14.,  15.,  19.],\n",
       "         [ 17.,  16.,  21.],\n",
       "         [244., 243., 248.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[255., 246., 254.],\n",
       "         [252., 242., 250.],\n",
       "         [240., 230., 238.],\n",
       "         ...,\n",
       "         [250., 244., 244.],\n",
       "         [250., 246., 245.],\n",
       "         [255., 251., 250.]],\n",
       "\n",
       "        [[228., 218., 226.],\n",
       "         [225., 215., 223.],\n",
       "         [224., 214., 222.],\n",
       "         ...,\n",
       "         [238., 234., 233.],\n",
       "         [253., 249., 248.],\n",
       "         [249., 245., 244.]],\n",
       "\n",
       "        [[233., 223., 231.],\n",
       "         [232., 222., 230.],\n",
       "         [232., 225., 232.],\n",
       "         ...,\n",
       "         [227., 223., 222.],\n",
       "         [240., 236., 235.],\n",
       "         [243., 242., 240.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[254., 252., 253.],\n",
       "         [255., 253., 254.],\n",
       "         [254., 254., 254.],\n",
       "         ...,\n",
       "         [253., 253., 253.],\n",
       "         [249., 249., 249.],\n",
       "         [246., 246., 246.]],\n",
       "\n",
       "        [[253., 251., 252.],\n",
       "         [255., 253., 254.],\n",
       "         [254., 254., 254.],\n",
       "         ...,\n",
       "         [253., 253., 253.],\n",
       "         [251., 251., 251.],\n",
       "         [249., 249., 249.]],\n",
       "\n",
       "        [[250., 248., 249.],\n",
       "         [253., 251., 252.],\n",
       "         [252., 252., 252.],\n",
       "         ...,\n",
       "         [249., 249., 249.],\n",
       "         [246., 246., 246.],\n",
       "         [244., 244., 244.]]],\n",
       "\n",
       "\n",
       "       [[[234., 235., 230.],\n",
       "         [241., 242., 237.],\n",
       "         [232., 232., 230.],\n",
       "         ...,\n",
       "         [228., 232., 233.],\n",
       "         [237., 241., 242.],\n",
       "         [236., 240., 241.]],\n",
       "\n",
       "        [[250., 251., 246.],\n",
       "         [250., 251., 246.],\n",
       "         [241., 241., 239.],\n",
       "         ...,\n",
       "         [222., 226., 227.],\n",
       "         [243., 247., 248.],\n",
       "         [249., 253., 254.]],\n",
       "\n",
       "        [[227., 227., 225.],\n",
       "         [218., 218., 216.],\n",
       "         [232., 232., 230.],\n",
       "         ...,\n",
       "         [198., 202., 201.],\n",
       "         [238., 242., 241.],\n",
       "         [248., 252., 251.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[211., 211., 209.],\n",
       "         [213., 213., 211.],\n",
       "         [223., 223., 221.],\n",
       "         ...,\n",
       "         [235., 235., 233.],\n",
       "         [252., 252., 250.],\n",
       "         [255., 255., 253.]],\n",
       "\n",
       "        [[219., 219., 217.],\n",
       "         [221., 221., 219.],\n",
       "         [228., 228., 226.],\n",
       "         ...,\n",
       "         [240., 240., 240.],\n",
       "         [244., 244., 244.],\n",
       "         [255., 255., 255.]],\n",
       "\n",
       "        [[214., 214., 212.],\n",
       "         [244., 244., 242.],\n",
       "         [217., 217., 215.],\n",
       "         ...,\n",
       "         [240., 240., 240.],\n",
       "         [248., 248., 248.],\n",
       "         [236., 236., 236.]]],\n",
       "\n",
       "\n",
       "       [[[252., 252., 252.],\n",
       "         [252., 252., 252.],\n",
       "         [252., 252., 252.],\n",
       "         ...,\n",
       "         [247., 247., 247.],\n",
       "         [244., 244., 244.],\n",
       "         [238., 238., 238.]],\n",
       "\n",
       "        [[252., 252., 252.],\n",
       "         [252., 252., 252.],\n",
       "         [252., 252., 252.],\n",
       "         ...,\n",
       "         [245., 245., 245.],\n",
       "         [243., 243., 243.],\n",
       "         [237., 237., 237.]],\n",
       "\n",
       "        [[252., 252., 252.],\n",
       "         [252., 252., 252.],\n",
       "         [253., 253., 253.],\n",
       "         ...,\n",
       "         [244., 244., 244.],\n",
       "         [243., 243., 243.],\n",
       "         [238., 238., 238.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[254., 254., 254.],\n",
       "         [254., 254., 254.],\n",
       "         [254., 254., 254.],\n",
       "         ...,\n",
       "         [249., 249., 249.],\n",
       "         [250., 250., 250.],\n",
       "         [247., 247., 247.]],\n",
       "\n",
       "        [[254., 254., 254.],\n",
       "         [254., 254., 254.],\n",
       "         [254., 254., 254.],\n",
       "         ...,\n",
       "         [248., 248., 248.],\n",
       "         [249., 249., 249.],\n",
       "         [247., 247., 247.]],\n",
       "\n",
       "        [[254., 254., 254.],\n",
       "         [254., 254., 254.],\n",
       "         [254., 254., 254.],\n",
       "         ...,\n",
       "         [249., 249., 249.],\n",
       "         [248., 248., 248.],\n",
       "         [247., 247., 247.]]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(train_df['image'].tolist())\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9ce1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[1].shape\n",
    "# take only (1/4,1/4) portion of the image since card can be classified by the top left corner\n",
    "x_train = x_train[:, :(int(x_train.shape[1]/4)), :(int(x_train.shape[2]/4)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82811faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#override the keras.models . Dense ....to add output from the dense layer is feeded to a variable...ie every Dense layer output is stored in a variable\n",
    "outputs_of_all_dense_layers = []\n",
    "def Dense(*args, **kwargs):\n",
    "    layer = keras.layers.Dense(*args, **kwargs)\n",
    "    def wrapper(input_tensor):\n",
    "        output_tensor = layer(input_tensor)\n",
    "        outputs_of_all_dense_layers.append(output_tensor)\n",
    "        return output_tensor\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a214d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1755961838.393616  293811 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1755961838.393644  293811 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93312</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">11,944,064</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,837</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93312\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m11,944,064\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m)             │         \u001b[38;5;34m6,837\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,951,797</span> (45.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,951,797\u001b[0m (45.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,951,797</span> (45.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,951,797\u001b[0m (45.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input_layer = Input(shape=(224, 224, 3))\n",
    "input_layer = Input(shape=(int(x_train.shape[1]), int(x_train.shape[2]), 3))\n",
    "covn_layer = keras.layers.Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "# covn_layer2 = keras.layers.MaxPooling2D((2, 2))(covn_layer)\n",
    "flatten_layer = keras.layers.Flatten()(covn_layer)\n",
    "hidden_layer_1 = Dense(128, activation='relu')(flatten_layer)\n",
    "hidden_layer_3 = Dense(53, activation='softmax')(hidden_layer_1)\n",
    "model=Model(inputs=input_layer, outputs=hidden_layer_3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5916705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9560 - loss: 73.2773 - val_accuracy: 0.0196 - val_loss: 20170.7031\n",
      "Epoch 2/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.9604 - loss: 69.4830 - val_accuracy: 0.0190 - val_loss: 20485.4863\n",
      "Epoch 3/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.9609 - loss: 63.0722 - val_accuracy: 0.0178 - val_loss: 19496.6465\n",
      "Epoch 4/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.9623 - loss: 84.5709 - val_accuracy: 0.0239 - val_loss: 23561.5059\n",
      "Epoch 5/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.9594 - loss: 81.0832 - val_accuracy: 0.0307 - val_loss: 19852.2480\n",
      "Epoch 6/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.9525 - loss: 118.6111 - val_accuracy: 0.0123 - val_loss: 23890.3984\n",
      "Epoch 7/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.9598 - loss: 84.9818 - val_accuracy: 0.0190 - val_loss: 22411.5488\n",
      "Epoch 8/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.9690 - loss: 48.7864 - val_accuracy: 0.0135 - val_loss: 24774.3125\n",
      "Epoch 9/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.9629 - loss: 91.4961 - val_accuracy: 0.0239 - val_loss: 21868.1445\n",
      "Epoch 10/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.9535 - loss: 107.7890 - val_accuracy: 0.0178 - val_loss: 25800.3359\n",
      "Training time: 42.044947147369385 seconds\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time \n",
    "start_time = time.time()\n",
    "history1 = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294520cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_of_all_dense_layers\n",
    "# print (outputs_of_all_dense_layers) for all layers\n",
    "print (outputs_of_all_dense_layers[-1])  # for last dense layer\n",
    "# <KerasTensor shape=(None, 53), dtype=float32, sparse=False, ragged=False, name=keras_tensor_34> print this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_numpy2)",
   "language": "python",
   "name": "tf_numpy2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
