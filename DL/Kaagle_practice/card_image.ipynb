{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a50ccef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow devices:\", tf.config.list_physical_devices())\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a574af90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages (0.3.12)\n",
      "Requirement already satisfied: packaging in /Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in /Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages (from kagglehub) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages (from requests->kagglehub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages (from requests->kagglehub) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/ajitesh/.cache/kagglehub/datasets/gpiosenka/cards-image-datasetclassification/versions/2\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"gpiosenka/cards-image-datasetclassification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28bda3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/ajitesh/Desktop/AI_ML_Learn/.venv/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %pip install tensorflow numpy pandas matplotlib seaborn scikit-learn\n",
    "from tensorflow import keras \n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099cbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import load_img\n",
    "import glob\n",
    "\n",
    "# Get all jpg image paths in the dataset directory (including subfolders)\n",
    "image_paths = glob.glob(os.path.join(path, '**', '*.jpg'), recursive=True)\n",
    "\n",
    "# Load images into a list\n",
    "# train_data = [load_img(img_path) for img_path in image_paths]\n",
    "train_data = []\n",
    "labels = []\n",
    "for i in image_paths:\n",
    "    train_data.append(load_img(i))\n",
    "    labels.append(i.split('/')[-2])  # Assuming the label is the folder name\n",
    "\n",
    "train_data[1].size\n",
    "# Convert to DataFrame\n",
    "train_df = pd.DataFrame({\n",
    "    'image': train_data,\n",
    "    'label': labels\n",
    "})\n",
    "train_df.head()\n",
    "# convert jpg images to tensors\n",
    "from keras.preprocessing.image import img_to_array\n",
    "train_df['image'] = train_df['image'].apply(lambda x: img_to_array(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f621c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(8154, 53))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Convert labels to categorical format\n",
    "coded_labels={\n",
    "\"ace of clubs\" :1       ,    \"eight of hearts\" :2  ,     \"four of clubs\" :3  ,          \"jack of hearts\" :4   ,       \"king of spades\" :5  ,        \"queen of diamonds\" :6       , \"seven of spades\" :7   , \"ten of diamonds\" :8         , \"three of spades\" :9,\n",
    "\"ace of diamonds\" :10     ,   \"eight of spades\" :11     ,    \"four of diamonds\" :12 ,       \"jack of spades\" :13   ,       \"nine of clubs\" :14    ,       \"queen of hearts\" :15         , \"six of clubs\" :16            , \"ten of hearts\" :17           , \"two of clubs\" :18,\n",
    "\"ace of hearts\" :19       ,   \"five of clubs\" :20       ,    \"four of hearts\" :21 ,         \"joker\" :22           ,        \"nine of diamonds\" :23 ,       \"queen of spades\" :24         , \"six of diamonds\" :25         , \"ten of spades\" :26           , \"two of diamonds\" :27,\n",
    "\"ace of spades\" :28       ,   \"five of diamonds\" :29    ,    \"four of spades\" :30 ,         \"king of clubs\" :31   ,        \"nine of hearts\" :32   ,       \"seven of clubs\" :33          , \"six of hearts\" :34           , \"three of clubs\" :35          , \"two of hearts\" :36,\n",
    "\"eight of clubs\" :37      ,   \"five of hearts\" :38      ,    \"jack of clubs\" :39 ,          \"king of diamonds\" :40   ,     \"nine of spades\" :41   ,      \"seven of diamonds\" :42       , \"six of spades\" :43           , \"three of diamonds\" :44       , \"two of spades\" :45,\n",
    "\"eight of diamonds\" :46   ,   \"five of spades\" :47      ,    \"jack of diamonds\" :48 ,       \"king of hearts\" :49   ,       \"queen of clubs\" :50   ,      \"seven of hearts\" :51        , \"ten of clubs\" :52            , \"three of hearts\" :53\n",
    "}\n",
    "def label_to_one_hot(label):\n",
    "    one_hot = np.zeros(53)\n",
    "    one_hot[coded_labels[label]-1] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train = np.array([label_to_one_hot(label) for label in train_df['label']])\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ea2ffb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[236., 221., 214.],\n",
       "         [235., 220., 213.],\n",
       "         [219., 204., 199.],\n",
       "         ...,\n",
       "         [220., 203., 195.],\n",
       "         [220., 203., 193.],\n",
       "         [221., 205., 192.]],\n",
       "\n",
       "        [[242., 227., 220.],\n",
       "         [240., 225., 218.],\n",
       "         [218., 203., 196.],\n",
       "         ...,\n",
       "         [219., 202., 194.],\n",
       "         [220., 203., 193.],\n",
       "         [221., 205., 192.]],\n",
       "\n",
       "        [[236., 221., 214.],\n",
       "         [239., 224., 217.],\n",
       "         [218., 203., 196.],\n",
       "         ...,\n",
       "         [220., 201., 194.],\n",
       "         [221., 203., 193.],\n",
       "         [222., 204., 194.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[186., 175., 173.],\n",
       "         [190., 179., 173.],\n",
       "         [199., 186., 177.],\n",
       "         ...,\n",
       "         [228., 209., 177.],\n",
       "         [226., 210., 177.],\n",
       "         [225., 209., 176.]],\n",
       "\n",
       "        [[187., 176., 180.],\n",
       "         [185., 175., 176.],\n",
       "         [187., 176., 170.],\n",
       "         ...,\n",
       "         [228., 208., 175.],\n",
       "         [231., 207., 173.],\n",
       "         [231., 205., 170.]],\n",
       "\n",
       "        [[190., 180., 188.],\n",
       "         [181., 172., 175.],\n",
       "         [176., 166., 164.],\n",
       "         ...,\n",
       "         [229., 206., 174.],\n",
       "         [232., 203., 169.],\n",
       "         [234., 201., 166.]]],\n",
       "\n",
       "\n",
       "       [[[244., 200., 223.],\n",
       "         [228., 186., 206.],\n",
       "         [234., 198., 212.],\n",
       "         ...,\n",
       "         [212., 186., 211.],\n",
       "         [228., 202., 229.],\n",
       "         [220., 194., 221.]],\n",
       "\n",
       "        [[224., 182., 204.],\n",
       "         [219., 180., 199.],\n",
       "         [226., 189., 206.],\n",
       "         ...,\n",
       "         [215., 189., 214.],\n",
       "         [222., 196., 223.],\n",
       "         [216., 190., 217.]],\n",
       "\n",
       "        [[220., 183., 201.],\n",
       "         [225., 188., 206.],\n",
       "         [226., 187., 206.],\n",
       "         ...,\n",
       "         [216., 190., 217.],\n",
       "         [218., 192., 219.],\n",
       "         [216., 190., 217.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[201., 191., 226.],\n",
       "         [209., 199., 234.],\n",
       "         [201., 191., 226.],\n",
       "         ...,\n",
       "         [189., 196., 248.],\n",
       "         [191., 200., 243.],\n",
       "         [195., 204., 245.]],\n",
       "\n",
       "        [[195., 185., 220.],\n",
       "         [208., 198., 233.],\n",
       "         [200., 190., 225.],\n",
       "         ...,\n",
       "         [194., 202., 249.],\n",
       "         [201., 205., 250.],\n",
       "         [194., 197., 238.]],\n",
       "\n",
       "        [[202., 192., 227.],\n",
       "         [207., 197., 232.],\n",
       "         [201., 191., 226.],\n",
       "         ...,\n",
       "         [195., 206., 252.],\n",
       "         [194., 197., 240.],\n",
       "         [205., 207., 248.]]],\n",
       "\n",
       "\n",
       "       [[[ 14.,   9.,  15.],\n",
       "         [ 18.,  13.,  19.],\n",
       "         [ 16.,  14.,  19.],\n",
       "         ...,\n",
       "         [ 11.,  10.,  15.],\n",
       "         [ 20.,  19.,  24.],\n",
       "         [253., 252., 255.]],\n",
       "\n",
       "        [[ 16.,  11.,  17.],\n",
       "         [ 12.,  10.,  15.],\n",
       "         [ 15.,  13.,  18.],\n",
       "         ...,\n",
       "         [ 12.,  11.,  16.],\n",
       "         [ 15.,  14.,  19.],\n",
       "         [ 14.,  13.,  18.]],\n",
       "\n",
       "        [[  9.,   7.,  10.],\n",
       "         [ 13.,  11.,  14.],\n",
       "         [ 10.,   8.,  13.],\n",
       "         ...,\n",
       "         [ 28.,  27.,  32.],\n",
       "         [ 16.,  15.,  20.],\n",
       "         [  8.,   7.,  12.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 17.,  18.,  23.],\n",
       "         [ 16.,  17.,  22.],\n",
       "         [ 19.,  20.,  25.],\n",
       "         ...,\n",
       "         [ 41.,  40.,  45.],\n",
       "         [ 42.,  41.,  46.],\n",
       "         [ 19.,  18.,  23.]],\n",
       "\n",
       "        [[ 20.,  21.,  26.],\n",
       "         [ 35.,  36.,  41.],\n",
       "         [ 13.,  14.,  19.],\n",
       "         ...,\n",
       "         [ 30.,  31.,  35.],\n",
       "         [ 93.,  92.,  97.],\n",
       "         [ 15.,  14.,  19.]],\n",
       "\n",
       "        [[ 16.,  17.,  22.],\n",
       "         [ 20.,  21.,  26.],\n",
       "         [ 19.,  20.,  25.],\n",
       "         ...,\n",
       "         [ 14.,  15.,  19.],\n",
       "         [ 17.,  16.,  21.],\n",
       "         [244., 243., 248.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[255., 246., 254.],\n",
       "         [252., 242., 250.],\n",
       "         [240., 230., 238.],\n",
       "         ...,\n",
       "         [250., 244., 244.],\n",
       "         [250., 246., 245.],\n",
       "         [255., 251., 250.]],\n",
       "\n",
       "        [[228., 218., 226.],\n",
       "         [225., 215., 223.],\n",
       "         [224., 214., 222.],\n",
       "         ...,\n",
       "         [238., 234., 233.],\n",
       "         [253., 249., 248.],\n",
       "         [249., 245., 244.]],\n",
       "\n",
       "        [[233., 223., 231.],\n",
       "         [232., 222., 230.],\n",
       "         [232., 225., 232.],\n",
       "         ...,\n",
       "         [227., 223., 222.],\n",
       "         [240., 236., 235.],\n",
       "         [243., 242., 240.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[254., 252., 253.],\n",
       "         [255., 253., 254.],\n",
       "         [254., 254., 254.],\n",
       "         ...,\n",
       "         [253., 253., 253.],\n",
       "         [249., 249., 249.],\n",
       "         [246., 246., 246.]],\n",
       "\n",
       "        [[253., 251., 252.],\n",
       "         [255., 253., 254.],\n",
       "         [254., 254., 254.],\n",
       "         ...,\n",
       "         [253., 253., 253.],\n",
       "         [251., 251., 251.],\n",
       "         [249., 249., 249.]],\n",
       "\n",
       "        [[250., 248., 249.],\n",
       "         [253., 251., 252.],\n",
       "         [252., 252., 252.],\n",
       "         ...,\n",
       "         [249., 249., 249.],\n",
       "         [246., 246., 246.],\n",
       "         [244., 244., 244.]]],\n",
       "\n",
       "\n",
       "       [[[234., 235., 230.],\n",
       "         [241., 242., 237.],\n",
       "         [232., 232., 230.],\n",
       "         ...,\n",
       "         [228., 232., 233.],\n",
       "         [237., 241., 242.],\n",
       "         [236., 240., 241.]],\n",
       "\n",
       "        [[250., 251., 246.],\n",
       "         [250., 251., 246.],\n",
       "         [241., 241., 239.],\n",
       "         ...,\n",
       "         [222., 226., 227.],\n",
       "         [243., 247., 248.],\n",
       "         [249., 253., 254.]],\n",
       "\n",
       "        [[227., 227., 225.],\n",
       "         [218., 218., 216.],\n",
       "         [232., 232., 230.],\n",
       "         ...,\n",
       "         [198., 202., 201.],\n",
       "         [238., 242., 241.],\n",
       "         [248., 252., 251.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[211., 211., 209.],\n",
       "         [213., 213., 211.],\n",
       "         [223., 223., 221.],\n",
       "         ...,\n",
       "         [235., 235., 233.],\n",
       "         [252., 252., 250.],\n",
       "         [255., 255., 253.]],\n",
       "\n",
       "        [[219., 219., 217.],\n",
       "         [221., 221., 219.],\n",
       "         [228., 228., 226.],\n",
       "         ...,\n",
       "         [240., 240., 240.],\n",
       "         [244., 244., 244.],\n",
       "         [255., 255., 255.]],\n",
       "\n",
       "        [[214., 214., 212.],\n",
       "         [244., 244., 242.],\n",
       "         [217., 217., 215.],\n",
       "         ...,\n",
       "         [240., 240., 240.],\n",
       "         [248., 248., 248.],\n",
       "         [236., 236., 236.]]],\n",
       "\n",
       "\n",
       "       [[[252., 252., 252.],\n",
       "         [252., 252., 252.],\n",
       "         [252., 252., 252.],\n",
       "         ...,\n",
       "         [247., 247., 247.],\n",
       "         [244., 244., 244.],\n",
       "         [238., 238., 238.]],\n",
       "\n",
       "        [[252., 252., 252.],\n",
       "         [252., 252., 252.],\n",
       "         [252., 252., 252.],\n",
       "         ...,\n",
       "         [245., 245., 245.],\n",
       "         [243., 243., 243.],\n",
       "         [237., 237., 237.]],\n",
       "\n",
       "        [[252., 252., 252.],\n",
       "         [252., 252., 252.],\n",
       "         [253., 253., 253.],\n",
       "         ...,\n",
       "         [244., 244., 244.],\n",
       "         [243., 243., 243.],\n",
       "         [238., 238., 238.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[254., 254., 254.],\n",
       "         [254., 254., 254.],\n",
       "         [254., 254., 254.],\n",
       "         ...,\n",
       "         [249., 249., 249.],\n",
       "         [250., 250., 250.],\n",
       "         [247., 247., 247.]],\n",
       "\n",
       "        [[254., 254., 254.],\n",
       "         [254., 254., 254.],\n",
       "         [254., 254., 254.],\n",
       "         ...,\n",
       "         [248., 248., 248.],\n",
       "         [249., 249., 249.],\n",
       "         [247., 247., 247.]],\n",
       "\n",
       "        [[254., 254., 254.],\n",
       "         [254., 254., 254.],\n",
       "         [254., 254., 254.],\n",
       "         ...,\n",
       "         [249., 249., 249.],\n",
       "         [248., 248., 248.],\n",
       "         [247., 247., 247.]]]], shape=(8154, 224, 224, 3), dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(train_df['image'].tolist())\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ce1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[1].shape\n",
    "# take only (1/4,1/4) portion of the image since card can be classified by the top left corner\n",
    "x_train = x_train[:, :(int(x_train.shape[1]/4)), :(int(x_train.shape[2]/4)), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82811faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#override the keras.models . Dense ....to add output from the dense layer is feeded to a variable...ie every Dense layer output is stored in a variable\n",
    "outputs_of_all_dense_layers = []\n",
    "def Dense(*args, **kwargs):\n",
    "    layer = keras.layers.Dense(*args, **kwargs)\n",
    "    def wrapper(input_tensor):\n",
    "        output_tensor = layer(input_tensor)\n",
    "        outputs_of_all_dense_layers.append(output_tensor)\n",
    "        return output_tensor\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a214d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93312</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">11,944,064</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,837</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93312\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m11,944,064\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m)             │         \u001b[38;5;34m6,837\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,951,797</span> (45.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,951,797\u001b[0m (45.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,951,797</span> (45.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,951,797\u001b[0m (45.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input_layer = Input(shape=(224, 224, 3))\n",
    "input_layer = Input(shape=(int(x_train.shape[1]), int(x_train.shape[2]), 3))\n",
    "covn_layer = keras.layers.Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "# covn_layer2 = keras.layers.MaxPooling2D((2, 2))(covn_layer)\n",
    "flatten_layer = keras.layers.Flatten()(covn_layer)\n",
    "hidden_layer_1 = Dense(128, activation='relu')(flatten_layer)\n",
    "hidden_layer_3 = Dense(53, activation='softmax')(hidden_layer_1)\n",
    "model=Model(inputs=input_layer, outputs=hidden_layer_3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5916705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.0300 - loss: 3.8980 - val_accuracy: 0.0000e+00 - val_loss: 5.2313\n",
      "Epoch 2/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.0294 - loss: 4.1207 - val_accuracy: 0.0000e+00 - val_loss: 5.3230\n",
      "Epoch 3/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.0296 - loss: 3.8656 - val_accuracy: 0.0000e+00 - val_loss: 5.4114\n",
      "Epoch 4/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.0297 - loss: 3.7981 - val_accuracy: 0.0000e+00 - val_loss: 5.4811\n",
      "Epoch 5/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.0297 - loss: 3.7873 - val_accuracy: 0.0000e+00 - val_loss: 5.5636\n",
      "Epoch 6/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.0300 - loss: 3.7963 - val_accuracy: 0.0000e+00 - val_loss: 5.6122\n",
      "Epoch 7/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.0297 - loss: 3.7835 - val_accuracy: 0.0000e+00 - val_loss: 5.6701\n",
      "Epoch 8/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.0299 - loss: 3.7817 - val_accuracy: 0.0000e+00 - val_loss: 5.7249\n",
      "Epoch 9/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.0299 - loss: 3.7809 - val_accuracy: 0.0000e+00 - val_loss: 5.7704\n",
      "Epoch 10/10\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.0300 - loss: 3.7790 - val_accuracy: 0.0000e+00 - val_loss: 5.8199\n",
      "Training time: 34.71039891242981 seconds\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "import time \n",
    "start_time = time.time()\n",
    "history1 = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "294520cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[-2.76348814e-02, -1.64568480e-02,  2.55110394e-02,\n",
       "            4.06737141e-02, -3.69843729e-02,  7.23663792e-02,\n",
       "            4.81192581e-02, -6.42326176e-02, -1.48371920e-01,\n",
       "            1.68562904e-02,  6.64894208e-02,  9.52886567e-02,\n",
       "           -5.42500354e-02, -1.29760265e-01,  7.90528283e-02,\n",
       "            2.93083955e-02, -8.49133059e-02,  3.98961380e-02,\n",
       "           -3.86255309e-02, -3.77098732e-02, -8.79632309e-03,\n",
       "            1.09760296e-02,  5.92303127e-02,  1.05049059e-01,\n",
       "           -6.26976639e-02,  1.02872096e-01, -1.08115263e-02,\n",
       "            1.30350292e-02,  6.33148942e-04, -1.31015955e-02,\n",
       "           -5.64208664e-02, -1.24753445e-01],\n",
       "          [-1.04928479e-01, -6.00326099e-02,  5.83244525e-02,\n",
       "           -1.35829836e-01, -1.73278246e-02, -1.05608292e-01,\n",
       "           -1.08598560e-01, -6.27258420e-02,  8.95872936e-02,\n",
       "           -1.42720059e-01, -6.03484213e-02, -1.44067258e-01,\n",
       "            3.08450982e-02,  5.06041013e-02, -6.96620718e-03,\n",
       "           -1.00998722e-01,  1.72859598e-02, -2.51356475e-02,\n",
       "           -9.97796953e-02, -1.14801966e-01, -1.65189505e-02,\n",
       "            5.17410822e-02, -2.28812010e-03, -1.42187655e-01,\n",
       "           -2.66832300e-02, -5.47972843e-02,  6.15747571e-02,\n",
       "           -4.11684215e-02, -2.25411658e-03, -9.47073102e-02,\n",
       "            1.09727107e-01, -7.65805505e-03],\n",
       "          [-4.14035423e-03,  5.34642152e-02,  1.81419763e-03,\n",
       "           -1.47409484e-01, -3.97212915e-02,  1.44647667e-02,\n",
       "           -4.74955104e-02, -1.58120226e-02, -7.21218511e-02,\n",
       "           -1.57023415e-01,  1.07159719e-01, -3.44396532e-02,\n",
       "           -4.31509092e-02, -7.96033591e-02, -8.48985165e-02,\n",
       "           -7.18974471e-02,  5.43746129e-02,  7.93553144e-02,\n",
       "            8.48767534e-02, -1.21066198e-01,  6.14661761e-02,\n",
       "            5.90901747e-02,  3.88869531e-02,  6.58736527e-02,\n",
       "           -5.67857958e-02, -1.01584248e-01,  5.76687139e-03,\n",
       "           -2.17842776e-02,  3.79851758e-02, -1.03050314e-01,\n",
       "            3.23914737e-02,  9.02951285e-02]],\n",
       " \n",
       "         [[ 8.16984326e-02, -1.51443556e-01, -3.39107327e-02,\n",
       "            9.52972546e-02, -1.40913054e-01,  1.92347374e-02,\n",
       "            9.63369012e-02, -1.30595177e-01, -7.46623948e-02,\n",
       "            5.97080812e-02, -1.55386239e-01, -1.13919057e-01,\n",
       "            3.66108902e-02,  4.47792038e-02, -1.30313531e-01,\n",
       "            1.15797311e-01,  7.20975623e-02, -3.05041056e-02,\n",
       "           -2.62721721e-03,  1.73978861e-02,  7.78051466e-02,\n",
       "           -2.02604569e-02, -8.40456560e-02,  6.83312910e-03,\n",
       "           -3.44133042e-02, -5.11748753e-02, -5.57752661e-02,\n",
       "           -8.31951052e-02,  7.51652643e-02,  5.85970134e-02,\n",
       "           -9.44505557e-02, -2.59968527e-02],\n",
       "          [ 9.36817974e-02, -2.79567894e-02,  1.07594289e-01,\n",
       "            4.62666340e-02,  8.56973454e-02, -2.99255662e-02,\n",
       "            9.97916088e-02, -9.70555022e-02,  8.26781839e-02,\n",
       "           -4.20827791e-02, -5.13384156e-02, -9.20044556e-02,\n",
       "            8.96159839e-03, -5.62488660e-02,  9.06711891e-02,\n",
       "           -1.22793883e-01, -5.06432652e-02,  2.70022564e-02,\n",
       "           -6.83607608e-02, -1.67270556e-01, -8.53801295e-02,\n",
       "           -1.89170167e-02,  6.82263002e-02, -2.49855649e-02,\n",
       "            4.91228141e-02, -9.64564160e-02,  7.51497969e-02,\n",
       "           -9.86116156e-02, -4.68788780e-02,  7.11151138e-02,\n",
       "           -5.30188791e-02,  5.37398644e-02],\n",
       "          [ 5.16691320e-02, -9.29654464e-02, -5.41155189e-02,\n",
       "            7.24868178e-02, -1.41690999e-01,  5.66686429e-02,\n",
       "           -1.10570788e-01, -8.05303262e-06, -8.49685669e-02,\n",
       "           -5.38083315e-02,  6.12973645e-02, -4.15483452e-02,\n",
       "           -1.06084414e-01, -1.46144226e-01,  2.18538493e-02,\n",
       "           -1.33371323e-01, -3.50785963e-02, -8.42342302e-02,\n",
       "            3.19401734e-02,  7.69506022e-02,  6.18433617e-02,\n",
       "           -3.84737104e-02, -1.02584893e-02, -4.83032614e-02,\n",
       "           -8.92015025e-02, -1.28086999e-01,  1.20883118e-02,\n",
       "           -7.61787267e-03,  3.14070396e-02,  1.97480079e-02,\n",
       "            1.10962085e-01, -2.39186566e-02]],\n",
       " \n",
       "         [[ 1.33595821e-02, -1.57478556e-01, -2.30423305e-02,\n",
       "           -5.72698787e-02,  1.39380544e-02, -3.71501707e-02,\n",
       "           -5.42251207e-02, -5.00397049e-02, -1.58710614e-01,\n",
       "           -8.24842751e-02, -1.28976569e-01, -6.13600463e-02,\n",
       "            6.47607818e-02, -8.97530913e-02, -2.44881376e-03,\n",
       "           -7.23977461e-02,  1.28231458e-02,  9.43004340e-02,\n",
       "            1.10350899e-01, -1.14295622e-02,  4.54150885e-02,\n",
       "           -2.98779178e-02, -9.89617705e-02, -4.18887436e-02,\n",
       "           -3.97710018e-02,  1.78883467e-02, -1.21190533e-01,\n",
       "           -1.79478601e-02,  2.29320638e-02, -1.44149335e-02,\n",
       "            1.35892287e-01, -7.31417723e-03],\n",
       "          [ 2.00725514e-02, -9.27896872e-02, -5.92663959e-02,\n",
       "           -8.32912102e-02,  4.76369597e-02, -6.44620135e-02,\n",
       "            9.74937975e-02,  3.97942662e-02, -1.44868299e-01,\n",
       "           -3.53689045e-02, -9.05077532e-02,  8.66574273e-02,\n",
       "            8.22029635e-03, -5.11895213e-03,  1.58830956e-02,\n",
       "            6.48798794e-02, -1.05647901e-02, -1.15130423e-02,\n",
       "           -1.25835210e-01, -1.23246178e-01, -1.04138412e-01,\n",
       "            6.89819902e-02, -7.81332478e-02,  6.95887208e-02,\n",
       "           -9.83876288e-02, -8.73847082e-02, -2.85639204e-02,\n",
       "           -1.37476832e-01,  8.47036839e-02,  5.14290631e-02,\n",
       "            1.29282251e-01,  2.64739152e-03],\n",
       "          [ 1.86840966e-02, -6.35470152e-02, -1.41229197e-01,\n",
       "           -1.21236779e-01,  7.64578134e-02, -4.38482538e-02,\n",
       "           -3.97966579e-02, -1.18640624e-02, -1.40950397e-01,\n",
       "           -3.12380474e-02, -9.10343826e-02, -8.08628723e-02,\n",
       "            4.00530361e-02,  1.10839196e-01, -1.32394567e-01,\n",
       "           -1.02208696e-01,  8.90966281e-02,  5.78949414e-02,\n",
       "           -1.75869852e-01,  8.27836320e-02,  6.56488016e-02,\n",
       "            4.75188754e-02, -1.23364203e-01, -1.09235253e-02,\n",
       "           -8.19437951e-03,  9.15637463e-02,  3.01533435e-02,\n",
       "           -1.24245241e-01, -1.32332444e-01,  3.73427309e-02,\n",
       "           -8.77112970e-02, -8.19609836e-02]]],\n",
       " \n",
       " \n",
       "        [[[-1.22643605e-01, -6.36683926e-02,  5.14627397e-02,\n",
       "           -2.59216912e-02, -6.49423078e-02, -1.64306447e-01,\n",
       "           -1.44297302e-01,  9.18906555e-02, -1.27547473e-01,\n",
       "           -4.87207435e-04, -1.24925770e-01,  9.59006026e-02,\n",
       "            4.60294820e-02,  4.82219681e-02, -3.65089253e-02,\n",
       "            1.04978581e-05, -2.02415995e-02, -1.23321995e-01,\n",
       "           -1.04234301e-01, -1.66928068e-01, -8.18124563e-02,\n",
       "           -6.92581311e-02,  8.29411671e-02, -1.52932420e-01,\n",
       "            3.35469954e-02, -6.05221093e-02, -3.09999492e-02,\n",
       "            6.53323084e-02,  7.77113885e-02,  9.15666372e-02,\n",
       "            8.85160565e-02, -1.32544085e-01],\n",
       "          [-5.65943383e-02,  3.80968601e-02, -3.67716588e-02,\n",
       "            1.11798525e-01, -1.28321856e-01, -1.69034764e-01,\n",
       "           -4.00228500e-02, -9.16529298e-02,  1.15648508e-02,\n",
       "           -5.33620492e-02,  2.28689960e-03, -6.58906922e-02,\n",
       "            3.10393684e-02,  3.90836075e-02,  1.38940644e-02,\n",
       "            1.16250612e-01, -1.01993568e-01, -3.61748897e-02,\n",
       "            3.85795161e-02,  1.02075048e-01, -1.12411730e-01,\n",
       "           -1.13083228e-01,  1.40889324e-02, -1.49508715e-01,\n",
       "            6.27554879e-02, -7.79671445e-02,  1.00199334e-01,\n",
       "            3.47747318e-02,  8.74234084e-03,  5.05145527e-02,\n",
       "            8.14122055e-03,  4.93854955e-02],\n",
       "          [-1.14095367e-01,  9.13558807e-03,  9.33559611e-02,\n",
       "            7.35712349e-02,  6.30435869e-02,  2.56172679e-02,\n",
       "            8.17646384e-02,  7.22505599e-02, -1.98419206e-03,\n",
       "            7.72830918e-02,  1.10477611e-01,  8.46905485e-02,\n",
       "           -3.73013727e-02, -1.53057680e-01,  6.12371191e-02,\n",
       "            6.35540113e-02, -1.01223625e-01, -6.35877028e-02,\n",
       "            8.21418092e-02, -9.58571583e-02, -8.03978443e-02,\n",
       "            9.93829034e-03,  7.53390193e-02,  1.90146286e-02,\n",
       "           -8.42025056e-02, -7.53010064e-02,  5.33587039e-02,\n",
       "           -1.44170389e-01, -6.95343316e-02, -1.30583182e-01,\n",
       "            1.28436135e-02, -3.17800976e-02]],\n",
       " \n",
       "         [[ 8.78224894e-02, -4.26297821e-03,  6.73037842e-02,\n",
       "            9.87682343e-02,  2.10254490e-02, -2.17465684e-02,\n",
       "           -1.36138663e-01,  9.53422934e-02,  3.37603549e-03,\n",
       "            7.20263347e-02, -7.84843862e-02, -1.62425339e-01,\n",
       "           -6.66365102e-02, -1.84914097e-02, -1.54632747e-01,\n",
       "           -4.78835478e-02, -1.15024567e-01, -6.26412034e-02,\n",
       "           -4.66037951e-02, -3.60223949e-02,  6.13794066e-02,\n",
       "            1.06700718e-01,  2.20893640e-02,  6.54987171e-02,\n",
       "           -3.45599167e-02, -4.06014472e-02,  5.79445921e-02,\n",
       "            6.58852309e-02,  9.42390263e-02, -6.90620989e-02,\n",
       "           -6.48363829e-02, -1.78601276e-02],\n",
       "          [-4.53642234e-02, -1.14634097e-01, -1.59772098e-01,\n",
       "           -1.17742993e-01, -1.33771732e-01, -1.28118813e-01,\n",
       "           -3.28411385e-02, -5.81783839e-02,  2.24709827e-02,\n",
       "            1.62657779e-02, -9.63395834e-02,  6.09926321e-02,\n",
       "            3.19221220e-03, -2.65502054e-02,  3.97885740e-02,\n",
       "           -3.32631506e-02, -7.22968951e-02,  7.87129402e-02,\n",
       "           -9.78852510e-02,  3.78675982e-02, -5.33177368e-02,\n",
       "           -6.89062327e-02, -1.10326752e-01,  4.74291965e-02,\n",
       "            2.95214858e-02,  3.43175642e-02,  9.10492912e-02,\n",
       "            6.52269050e-02, -4.97799274e-03,  3.67923416e-02,\n",
       "           -1.07838735e-01,  9.01796222e-02],\n",
       "          [ 8.86610821e-02,  1.76341571e-02, -1.10198826e-01,\n",
       "            8.47074017e-02,  1.52434101e-02,  9.23865754e-03,\n",
       "           -1.42819822e-01, -1.33312479e-01, -2.86560543e-02,\n",
       "           -4.38862070e-02,  9.21185389e-02,  2.54518315e-02,\n",
       "            4.86892499e-02, -1.04034655e-02, -6.54881028e-03,\n",
       "           -4.89026904e-02, -5.56506813e-02, -6.75986782e-02,\n",
       "           -1.55924678e-01,  4.23681922e-02,  6.49388582e-02,\n",
       "           -1.35733008e-01,  3.24096531e-02, -7.65280128e-02,\n",
       "            4.66374569e-02,  1.35717001e-02, -5.42922132e-02,\n",
       "            2.67776996e-02,  8.42022002e-02,  6.38492405e-02,\n",
       "            4.08631712e-02,  9.79958847e-02]],\n",
       " \n",
       "         [[ 1.50004430e-02, -4.54132026e-03,  8.52514878e-02,\n",
       "           -8.47371519e-02, -1.38781697e-01, -7.31337294e-02,\n",
       "            1.81172919e-02, -4.19283025e-02,  4.87015769e-03,\n",
       "            9.22310445e-03, -1.06486820e-01,  1.04382075e-01,\n",
       "           -1.50081739e-02,  5.63272182e-03, -6.03952212e-03,\n",
       "           -8.38505775e-02,  7.73294717e-02, -9.36633497e-02,\n",
       "           -1.32317245e-01, -8.60810280e-03,  9.86853465e-02,\n",
       "            2.92482413e-03, -1.35221869e-01, -7.21155331e-02,\n",
       "            1.11126207e-01, -8.48438293e-02, -2.23242752e-02,\n",
       "           -2.79296320e-02,  4.14885879e-02, -2.51477789e-02,\n",
       "            1.12729566e-02,  8.03857893e-02],\n",
       "          [-7.72282854e-02,  4.58932184e-02,  3.63466479e-02,\n",
       "           -6.48671910e-02,  9.98487920e-02,  9.38001946e-02,\n",
       "            1.04230911e-01, -3.07329721e-03,  7.84304291e-02,\n",
       "            1.01077296e-01,  3.10491468e-03,  1.43576153e-02,\n",
       "           -1.24596231e-01, -2.93274354e-02, -1.33501559e-01,\n",
       "           -3.95280384e-02, -1.51267350e-01, -1.49255171e-02,\n",
       "            3.99755277e-02, -1.52597904e-01, -8.76370668e-02,\n",
       "           -3.84746231e-02,  1.11869676e-02, -6.43801838e-02,\n",
       "           -1.49118736e-01,  8.96603912e-02, -9.87820700e-02,\n",
       "           -9.09401756e-03,  4.06525955e-02,  6.63807765e-02,\n",
       "            8.59780535e-02, -3.94197293e-02],\n",
       "          [-1.38748139e-01, -8.55003819e-02,  5.35088554e-02,\n",
       "           -3.12849693e-02, -9.88462269e-02,  9.95182097e-02,\n",
       "            6.23291209e-02, -1.24050960e-01,  7.72411972e-02,\n",
       "           -1.94635820e-02, -1.19302079e-01,  1.13240182e-02,\n",
       "           -4.81040440e-02, -7.74906157e-03, -7.22948760e-02,\n",
       "            1.11429907e-01,  1.76198743e-02, -1.45783886e-01,\n",
       "           -1.06560841e-01, -3.49251591e-02,  8.39576423e-02,\n",
       "            2.64376905e-02, -9.25683528e-02,  7.11237490e-02,\n",
       "           -3.71022150e-02, -4.17331047e-02, -2.59277578e-02,\n",
       "           -3.13933827e-02,  7.88836256e-02, -4.03278843e-02,\n",
       "            6.62009406e-04, -2.62304544e-02]]],\n",
       " \n",
       " \n",
       "        [[[-1.03163250e-01, -1.48551732e-01, -8.26720819e-02,\n",
       "           -2.76671331e-02, -1.10638693e-01,  9.54045579e-02,\n",
       "            9.92077589e-02,  1.15883142e-01,  3.05156969e-02,\n",
       "            7.32149109e-02,  5.89507930e-02, -2.35917978e-02,\n",
       "            4.62554283e-02, -1.43388249e-02, -1.32227957e-01,\n",
       "           -9.30971429e-02, -5.73864505e-02, -1.45355463e-01,\n",
       "           -5.00265229e-03, -1.38321310e-01, -8.60986933e-02,\n",
       "           -1.48295194e-01, -1.51540324e-01, -7.62775764e-02,\n",
       "           -1.16728753e-01, -1.93767843e-03,  1.90740805e-02,\n",
       "           -9.06180590e-03, -1.27784237e-01, -6.49470314e-02,\n",
       "           -3.01773045e-02,  1.01024844e-01],\n",
       "          [-2.78663374e-02,  2.95146033e-02, -1.42875195e-01,\n",
       "            8.28857496e-02,  6.14334047e-02,  7.00607076e-02,\n",
       "            3.84394228e-02,  8.95274878e-02,  3.25140506e-02,\n",
       "            3.63602326e-03,  8.22183266e-02, -1.30009919e-01,\n",
       "           -1.07426248e-01,  8.08249339e-02, -1.15278615e-02,\n",
       "            1.14035055e-01, -8.02366659e-02, -9.26101878e-02,\n",
       "           -1.07683234e-01, -1.01964466e-01,  2.52607558e-02,\n",
       "            1.25230029e-01, -8.72621220e-03, -8.38201046e-02,\n",
       "            1.15623504e-01,  3.04651679e-03, -1.56005681e-01,\n",
       "            1.44811274e-04, -8.85935947e-02, -1.21415235e-01,\n",
       "            3.72040048e-02,  3.68190855e-02],\n",
       "          [ 6.81527033e-02,  4.67123464e-02,  2.12085503e-03,\n",
       "            8.81542787e-02,  1.08802393e-01,  4.35870104e-02,\n",
       "           -5.10626696e-02,  1.60726812e-02, -4.62690089e-03,\n",
       "           -7.99001306e-02, -9.48230848e-02, -1.61992818e-01,\n",
       "           -1.02718964e-01, -9.75344256e-02, -1.06644832e-01,\n",
       "           -1.90305393e-02, -1.02376781e-01,  6.57969862e-02,\n",
       "            9.16989073e-02, -8.69906098e-02,  2.50371620e-02,\n",
       "            8.14863481e-03, -9.65119302e-02,  1.43326372e-02,\n",
       "           -1.16275415e-01, -2.14897767e-02,  3.56672890e-02,\n",
       "           -9.06502530e-02, -8.81672353e-02, -5.86717725e-02,\n",
       "           -9.79102999e-02,  2.38201786e-02]],\n",
       " \n",
       "         [[ 6.64217323e-02, -8.03645700e-02, -9.17459726e-02,\n",
       "           -1.16993666e-01,  4.17065695e-02,  1.00302242e-01,\n",
       "            4.66032512e-02, -3.72015983e-02, -3.24283876e-02,\n",
       "           -4.38050192e-04,  1.02897950e-01,  5.39561175e-02,\n",
       "           -9.29162651e-02, -5.30207902e-02,  6.86513782e-02,\n",
       "           -6.76423311e-02,  7.07884058e-02, -1.32931679e-01,\n",
       "           -7.69514889e-02, -1.35303572e-01, -5.50648682e-02,\n",
       "           -6.77136108e-02, -5.10880873e-02,  6.50162771e-02,\n",
       "            8.36664960e-02,  7.42293894e-02,  5.10949418e-02,\n",
       "            6.48759231e-02, -7.01071844e-02,  5.47283180e-02,\n",
       "           -1.94570646e-02, -1.40177667e-01],\n",
       "          [-1.31287694e-01, -5.43818288e-02,  8.37819576e-02,\n",
       "           -7.20249265e-02, -8.78643841e-02,  2.96608210e-02,\n",
       "           -6.49706274e-02, -9.49328542e-02, -3.69481221e-02,\n",
       "            1.08121842e-01,  1.75183043e-02, -5.18741459e-02,\n",
       "           -1.24878906e-01,  9.91193205e-02, -9.18548182e-03,\n",
       "            8.55423659e-02, -8.11481103e-03, -8.64653736e-02,\n",
       "            3.24510410e-02, -6.16702512e-02, -2.15534195e-02,\n",
       "           -2.73346361e-02, -5.24941348e-02, -3.11711878e-02,\n",
       "           -6.25561699e-02,  3.41299325e-02,  3.59192351e-03,\n",
       "           -1.29907951e-01, -4.49651387e-03, -2.69391220e-02,\n",
       "           -5.87497316e-02, -5.44442460e-02],\n",
       "          [ 9.70693976e-02, -5.03852172e-03, -4.71873907e-03,\n",
       "           -2.61742137e-02, -1.23847820e-01,  7.18501210e-02,\n",
       "           -1.35680959e-01, -1.31927490e-01,  1.31698966e-03,\n",
       "           -8.98164958e-02, -5.92801312e-04,  7.30049685e-02,\n",
       "           -6.60313517e-02,  1.76908094e-02, -1.12969041e-01,\n",
       "           -7.98750520e-02, -4.83056195e-02, -1.10307842e-01,\n",
       "           -1.21165253e-02,  9.77754295e-02, -1.10744372e-01,\n",
       "           -7.44028762e-02, -5.17010540e-02, -9.25261900e-03,\n",
       "           -9.53840092e-02, -2.00161599e-02,  3.40621243e-03,\n",
       "            8.80018575e-04, -2.56186482e-02, -7.44362622e-02,\n",
       "           -5.57455122e-02, -9.18123201e-02]],\n",
       " \n",
       "         [[-2.99820099e-02, -6.47181422e-02, -6.93950430e-02,\n",
       "            1.04648307e-01, -8.31006318e-02, -9.13753808e-02,\n",
       "           -3.04845329e-02, -9.49543118e-02, -1.12625852e-01,\n",
       "            8.26521218e-02,  4.38304767e-02, -3.61772105e-02,\n",
       "            3.08323316e-02,  3.51098441e-02,  1.16992397e-02,\n",
       "            6.21213205e-02, -6.40144898e-03, -1.14468195e-01,\n",
       "           -1.23249687e-01, -1.54664263e-01,  2.37958468e-02,\n",
       "           -2.45690029e-02, -3.01965065e-02,  5.33715226e-02,\n",
       "            3.39781344e-02,  3.68644558e-02, -7.32149258e-02,\n",
       "           -8.52035210e-02, -2.54816413e-02,  4.70255315e-02,\n",
       "           -9.51054394e-02, -1.21696144e-01],\n",
       "          [-1.19072028e-01, -5.35384081e-02,  1.57310870e-02,\n",
       "           -7.18027279e-02,  1.50441667e-02,  1.01285996e-02,\n",
       "            9.98933837e-02,  8.45681280e-02, -5.43233901e-02,\n",
       "           -9.89605263e-02,  7.58985803e-02,  1.07778043e-01,\n",
       "            6.73509613e-02, -1.44951195e-01,  1.99634284e-02,\n",
       "            7.43076429e-02, -1.48453012e-01,  5.72329275e-02,\n",
       "            5.07904030e-02, -1.16052471e-01,  4.69645225e-02,\n",
       "            5.78794554e-02,  1.03430972e-01, -7.86389261e-02,\n",
       "            1.13606006e-01, -9.63772088e-02, -7.18931034e-02,\n",
       "           -6.61883801e-02, -1.01859987e-01, -9.06884000e-02,\n",
       "            9.90186110e-02, -8.89517441e-02],\n",
       "          [ 6.68812618e-02,  9.81017426e-02,  6.41694739e-02,\n",
       "           -9.99820158e-02,  2.84122806e-02, -3.53454612e-02,\n",
       "           -1.05853066e-01, -1.52311966e-01, -1.13423169e-01,\n",
       "           -1.37988314e-01,  5.71181998e-02,  9.15319920e-02,\n",
       "           -1.54664010e-01, -1.06361426e-01, -1.56570628e-01,\n",
       "           -4.07916047e-02,  3.41751166e-02,  1.07228912e-01,\n",
       "           -1.67482242e-01,  5.31380391e-03, -1.09696120e-01,\n",
       "            1.05174575e-02,  1.00519769e-01, -5.20832501e-02,\n",
       "           -1.38759717e-01, -7.53304213e-02,  3.85671407e-02,\n",
       "           -1.93361323e-02,  9.36885551e-02,  9.57550108e-03,\n",
       "            3.31280641e-02,  1.12703510e-01]]]], dtype=float32),\n",
       " array([-0.02508557, -0.02873948, -0.02805929, -0.02565471, -0.0282628 ,\n",
       "        -0.0353132 , -0.02419742, -0.02005946, -0.0254485 , -0.03262936,\n",
       "        -0.02611952, -0.02918623, -0.03174349,  0.0541678 , -0.02964834,\n",
       "        -0.02074241, -0.03408546, -0.02251121, -0.01196086, -0.01154663,\n",
       "        -0.03261984, -0.0179091 , -0.02287895, -0.03128193, -0.02658932,\n",
       "        -0.03193195, -0.03370408, -0.03077006, -0.03376795, -0.03357314,\n",
       "         0.05384322, -0.02005222], dtype=float32),\n",
       " array([[-0.00695146, -0.01054099, -0.00977706, ..., -0.00869896,\n",
       "         -0.00274602, -0.00676274],\n",
       "        [-0.004772  , -0.002742  ,  0.00086943, ...,  0.00642737,\n",
       "         -0.00312416,  0.0056621 ],\n",
       "        [-0.01295796, -0.00393534, -0.01102874, ..., -0.00247507,\n",
       "         -0.00075022, -0.00486541],\n",
       "        ...,\n",
       "        [-0.01398092, -0.00747312, -0.01026373, ..., -0.00352153,\n",
       "         -0.00548931, -0.00109693],\n",
       "        [-0.00648619, -0.0007505 , -0.00251178, ..., -0.00226651,\n",
       "         -0.01953277,  0.00457006],\n",
       "        [-0.01100772, -0.00034918, -0.0123562 , ..., -0.00022956,\n",
       "          0.00225941,  0.00095751]], shape=(93312, 128), dtype=float32),\n",
       " array([-0.00616072, -0.00642608, -0.00620536, -0.00638999, -0.00607168,\n",
       "        -0.00502143, -0.01431838, -0.00600521, -0.01520341, -0.00600526,\n",
       "        -0.00600048, -0.00600516, -0.00600535, -0.00451377, -0.01312247,\n",
       "        -0.00447581, -0.00812634, -0.00313079, -0.00373459, -0.01956504,\n",
       "        -0.00513284, -0.00258231, -0.00407965, -0.00600334, -0.00697728,\n",
       "        -0.00600263, -0.00509252, -0.01538559, -0.03290754, -0.01113378,\n",
       "        -0.00209161, -0.00526857, -0.00600531, -0.00373896, -0.01559262,\n",
       "        -0.01123483, -0.00600284, -0.00411771, -0.00598225, -0.00600517,\n",
       "        -0.01235955, -0.0067092 , -0.00639361, -0.00600495, -0.00599834,\n",
       "        -0.00215796, -0.00719647, -0.01669024, -0.00600033, -0.00318202,\n",
       "        -0.00627455, -0.0079711 , -0.00710143, -0.0040612 , -0.00353071,\n",
       "        -0.00600508, -0.00743987, -0.00654153, -0.00600494, -0.00417578,\n",
       "        -0.00562488, -0.01051684, -0.00600511, -0.00526207, -0.00178008,\n",
       "        -0.00600525, -0.00669916, -0.01607013, -0.00758245, -0.01499247,\n",
       "        -0.00687415, -0.00698302, -0.00650142, -0.00627069, -0.0039105 ,\n",
       "        -0.00284044,  0.01698966, -0.00268663, -0.01485978, -0.00498356,\n",
       "        -0.00414127, -0.00856658, -0.00543192, -0.02474084, -0.00600524,\n",
       "        -0.00602471, -0.00637856, -0.00471376, -0.00621945, -0.0060051 ,\n",
       "        -0.02571011, -0.00445105, -0.00600376, -0.00653399, -0.00698152,\n",
       "        -0.00741226, -0.00867262, -0.00351351, -0.00590511, -0.00583271,\n",
       "        -0.00600528, -0.00622224, -0.01877533, -0.0079657 , -0.00599956,\n",
       "        -0.00776566, -0.01417783, -0.00020036, -0.00600528, -0.00600532,\n",
       "        -0.01137436, -0.00674608, -0.00653818, -0.01431839,  0.00682589,\n",
       "        -0.00600529, -0.01200601, -0.00322159, -0.01629553, -0.01016595,\n",
       "        -0.00630954, -0.00428883, -0.00600532, -0.00588469, -0.01484651,\n",
       "        -0.00746152, -0.0235919 , -0.01044897], dtype=float32),\n",
       " array([[ 0.00209163,  0.16123022,  0.12084053, ...,  0.05942154,\n",
       "         -0.1122455 , -0.0345479 ],\n",
       "        [ 0.1656045 ,  0.0616615 ,  0.01898547, ...,  0.11831792,\n",
       "          0.14938216,  0.1316915 ],\n",
       "        [ 0.17505988,  0.01799189,  0.14141864, ...,  0.05383281,\n",
       "          0.01406282, -0.08029338],\n",
       "        ...,\n",
       "        [ 0.18136391, -0.01779632, -0.08838968, ..., -0.13721547,\n",
       "         -0.12047365, -0.02170346],\n",
       "        [ 0.07498734, -0.08560172, -0.03686257, ...,  0.07229378,\n",
       "         -0.16145898, -0.05498596],\n",
       "        [ 0.01725164, -0.14426771,  0.12399269, ..., -0.19656436,\n",
       "         -0.18300489, -0.11912138]], shape=(128, 53), dtype=float32),\n",
       " array([ 0.02379758, -0.81078696,  0.19184573,  0.23500766,  0.16286069,\n",
       "         0.21760641,  0.24126014,  0.19189322, -0.92195696, -1.2882599 ,\n",
       "         0.09962963, -1.2046689 ,  0.22605415,  0.04330502,  0.1292784 ,\n",
       "         0.17988327,  0.06032843, -1.2637604 ,  0.2352562 , -1.2570906 ,\n",
       "         0.18137906, -0.02128712,  0.05388105,  0.21143343,  0.12410746,\n",
       "         0.23446444, -1.2814063 ,  0.2808981 ,  0.10638838,  0.14421669,\n",
       "        -1.2033135 ,  0.0680934 , -0.04624548,  0.0501901 ,  0.04402614,\n",
       "         0.19416814,  0.09070361,  0.0934316 ,  0.25057402, -1.2749506 ,\n",
       "        -1.2809129 , -1.2580417 ,  0.19404669,  0.21113668,  0.19109261,\n",
       "         0.18877882,  0.19891798,  0.18677415,  0.05634253,  0.21694781,\n",
       "        -1.2764132 ,  0.11141405, -0.02608502], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2de3702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KerasTensor shape=(None, 53), dtype=float32, sparse=False, ragged=False, name=keras_tensor_4>\n"
     ]
    }
   ],
   "source": [
    "outputs_of_all_dense_layers\n",
    "# print (outputs_of_all_dense_layers) for all layers\n",
    "print (outputs_of_all_dense_layers[-1])  # for last dense layer\n",
    "# <KerasTensor shape=(None, 53), dtype=float32, sparse=False, ragged=False, name=keras_tensor_34> print this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
