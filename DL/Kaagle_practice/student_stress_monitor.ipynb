{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b74c9c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"StressLevelModel\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"StressLevelModel\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m1,050\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_layer (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,101</span> (4.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,101\u001b[0m (4.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,101</span> (4.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,101\u001b[0m (4.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model architecture saved as 'model_architecture.png'\n"
     ]
    }
   ],
   "source": [
    "# Create a new model with explicit layer definitions as requested\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=(x_train.shape[1],), name='input_layer')\n",
    "hidden_layer = Dense(50, activation='relu', name='hidden_layer')(input_layer)\n",
    "dropout_layer = Dropout(0.3, name='dropout_layer')(hidden_layer)\n",
    "# Output layer\n",
    "output_layer = Dense(1, activation='linear', name='output_layer')(dropout_layer)\n",
    "model_new = Model(inputs=input_layer, outputs=output_layer, name='StressLevelModel')\n",
    "# Compile the model\n",
    "model_new.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'accuracy'])\n",
    "# Display model architecture\n",
    "print(\"Model Architecture:\")\n",
    "model_new.summary()\n",
    "\n",
    "# Visualize model architecture\n",
    "from keras.utils import plot_model\n",
    "plot_model(model_new, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "print(\"\\nModel architecture saved as 'model_architecture.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be3f533a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pred = model_new.predict(\u001b[43mx_test\u001b[49m[\u001b[32m1\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "pred = model_new.predict(x_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f0a35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3decfa90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11527926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa8deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create intermediate models for layer output visualization\n",
    "def create_layer_output_models(model):\n",
    "    \"\"\"Create models that output intermediate layer results\"\"\"\n",
    "    layer_outputs = {}\n",
    "    \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if layer.name != 'input_layer':  # Skip input layer\n",
    "            # Create a model that outputs up to this layer\n",
    "            intermediate_model = Model(inputs=model.input, outputs=layer.output)\n",
    "            layer_outputs[layer.name] = intermediate_model\n",
    "    \n",
    "    return layer_outputs\n",
    "\n",
    "# Function to visualize outputs from each layer\n",
    "def visualize_layer_outputs(model, sample_data, layer_output_models, sample_indices=[0, 1, 2]):\n",
    "    \"\"\"Visualize outputs from each layer for sample data points\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"LAYER OUTPUT VISUALIZATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for sample_idx in sample_indices:\n",
    "        print(f\"\\n--- Sample {sample_idx + 1} ---\")\n",
    "        sample_input = sample_data[sample_idx:sample_idx+1]  # Keep batch dimension\n",
    "        \n",
    "        print(f\"Input shape: {sample_input.shape}\")\n",
    "        print(f\"Input values (first 10): {sample_input.flatten()[:10]}\")\n",
    "        \n",
    "        for layer_name, layer_model in layer_output_models.items():\n",
    "            output = layer_model.predict(sample_input, verbose=0)\n",
    "            print(f\"\\n{layer_name}:\")\n",
    "            print(f\"  Output shape: {output.shape}\")\n",
    "            print(f\"  Output values: {output.flatten()}\")\n",
    "            \n",
    "            # For hidden layers, show statistics\n",
    "            if 'hidden' in layer_name.lower():\n",
    "                print(f\"  Mean: {np.mean(output):.4f}\")\n",
    "                print(f\"  Std: {np.std(output):.4f}\")\n",
    "                print(f\"  Min: {np.min(output):.4f}\")\n",
    "                print(f\"  Max: {np.max(output):.4f}\")\n",
    "                print(f\"  Active nodes (>0): {np.sum(output > 0)} / {output.shape[-1]}\")\n",
    "\n",
    "# Function to visualize individual node outputs from hidden layers\n",
    "def visualize_hidden_nodes(model, sample_data, layer_output_models, sample_indices=[0, 1, 2]):\n",
    "    \"\"\"Visualize individual node outputs from hidden layers\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"HIDDEN LAYER NODE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for layer_name, layer_model in layer_output_models.items():\n",
    "        if 'hidden' in layer_name.lower():\n",
    "            print(f\"\\n--- {layer_name.upper()} NODES ---\")\n",
    "            \n",
    "            # Get outputs for all sample indices\n",
    "            outputs = []\n",
    "            for sample_idx in sample_indices:\n",
    "                sample_input = sample_data[sample_idx:sample_idx+1]\n",
    "                output = layer_model.predict(sample_input, verbose=0)\n",
    "                outputs.append(output.flatten())\n",
    "            \n",
    "            outputs = np.array(outputs)  # Shape: (num_samples, num_nodes)\n",
    "            \n",
    "            # Create visualization\n",
    "            plt.figure(figsize=(15, 6))\n",
    "            \n",
    "            # Plot 1: Heatmap of node activations\n",
    "            plt.subplot(1, 3, 1)\n",
    "            sns.heatmap(outputs, annot=True, fmt='.3f', cmap='RdYlBu_r', \n",
    "                       xticklabels=[f'Node_{i+1}' for i in range(outputs.shape[1])],\n",
    "                       yticklabels=[f'Sample_{i+1}' for i in range(outputs.shape[0])])\n",
    "            plt.title(f'{layer_name} - Node Activations')\n",
    "            plt.ylabel('Samples')\n",
    "            plt.xlabel('Nodes')\n",
    "            \n",
    "            # Plot 2: Node activation distribution\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.boxplot([outputs[:, i] for i in range(outputs.shape[1])], \n",
    "                       labels=[f'N{i+1}' for i in range(outputs.shape[1])])\n",
    "            plt.title(f'{layer_name} - Activation Distribution')\n",
    "            plt.ylabel('Activation Value')\n",
    "            plt.xlabel('Node')\n",
    "            plt.xticks(rotation=45)\n",
    "            \n",
    "            # Plot 3: Average activation per node\n",
    "            plt.subplot(1, 3, 3)\n",
    "            mean_activations = np.mean(outputs, axis=0)\n",
    "            bars = plt.bar(range(len(mean_activations)), mean_activations, \n",
    "                          color=['red' if x < 0 else 'blue' for x in mean_activations])\n",
    "            plt.title(f'{layer_name} - Average Activations')\n",
    "            plt.ylabel('Mean Activation')\n",
    "            plt.xlabel('Node')\n",
    "            plt.xticks(range(len(mean_activations)), [f'N{i+1}' for i in range(len(mean_activations))], rotation=45)\n",
    "            plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{layer_name}_node_analysis.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            # Print node statistics\n",
    "            print(f\"\\nNode Statistics for {layer_name}:\")\n",
    "            for i in range(outputs.shape[1]):\n",
    "                node_values = outputs[:, i]\n",
    "                print(f\"  Node {i+1}: Mean={np.mean(node_values):.4f}, \"\n",
    "                      f\"Std={np.std(node_values):.4f}, \"\n",
    "                      f\"Min={np.min(node_values):.4f}, \"\n",
    "                      f\"Max={np.max(node_values):.4f}\")\n",
    "\n",
    "print(\"Layer visualization functions created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36415776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the new model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train_split, y_test = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler_new = StandardScaler()\n",
    "X_train_scaled = scaler_new.fit_transform(X_train)\n",
    "X_test_scaled = scaler_new.transform(X_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "print(f\"Target distribution in training set:\")\n",
    "print(pd.Series(y_train_split).value_counts().sort_index())\n",
    "\n",
    "# Train the new model\n",
    "print(\"\\nTraining the new model...\")\n",
    "history_new = model_new.fit(\n",
    "    X_train_scaled, y_train_split, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history_new.history['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history_new.history['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history_new.history['mae'], label='Training MAE', color='blue')\n",
    "plt.plot(history_new.history['val_mae'], label='Validation MAE', color='orange')\n",
    "plt.title('Mean Absolute Error')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Make predictions\n",
    "y_pred = model_new.predict(X_test_scaled)\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('True vs Predicted Values')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_training_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nModel training completed!\")\n",
    "print(f\"Final training loss: {history_new.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final validation loss: {history_new.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final training MAE: {history_new.history['mae'][-1]:.4f}\")\n",
    "print(f\"Final validation MAE: {history_new.history['val_mae'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create intermediate models for layer output visualization\n",
    "layer_output_models = create_layer_output_models(model_new)\n",
    "\n",
    "print(\"Created intermediate models for layers:\")\n",
    "for layer_name in layer_output_models.keys():\n",
    "    print(f\"  - {layer_name}\")\n",
    "\n",
    "# Select sample data points from different stress levels\n",
    "sample_indices = []\n",
    "for stress_level in sorted(y_test.unique()):\n",
    "    # Get first occurrence of each stress level\n",
    "    idx = y_test[y_test == stress_level].index[0]\n",
    "    # Convert to position in test set\n",
    "    test_position = list(y_test.index).index(idx)\n",
    "    sample_indices.append(test_position)\n",
    "\n",
    "print(f\"\\nSelected sample indices: {sample_indices}\")\n",
    "print(\"Corresponding stress levels:\", [y_test.iloc[i] for i in sample_indices])\n",
    "\n",
    "# Visualize layer outputs\n",
    "visualize_layer_outputs(model_new, X_test_scaled, layer_output_models, sample_indices)\n",
    "\n",
    "# Visualize individual node outputs from hidden layers\n",
    "visualize_hidden_nodes(model_new, X_test_scaled, layer_output_models, sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fea0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mdsultanulislamovi/student-stress-monitoring-datasets\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Analysis of Stress Level Dataset for Machine Learning\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "# Custom color palettes - choose one or mix them\n",
    "blue_palette = sns.color_palette(\"Blues_r\", n_colors=8)\n",
    "green_palette = sns.color_palette(\"Greens_r\", n_colors=8)\n",
    "orange_palette = sns.color_palette(\"Oranges_r\", n_colors=8)\n",
    "# Set a blue-green palette as default\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA LOADING AND INITIAL EXPLORATION\n",
    "# =============================================================================\n",
    "\n",
    "# Load the dataset\n",
    "import os\n",
    "csv_path = os.path.join(path, \"StressLevelDataset.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Total Records: {df.shape[0]}\")\n",
    "\n",
    "print(f\"Total Features: {df.shape[1]}\")\n",
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "x_train = df.drop(columns=['stress_level'])\n",
    "y_train = df['stress_level']\n",
    "\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3936c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(df.shape[1]-1,)))\n",
    "model.add(Dense(50,activation='linear'))\n",
    "# the best activation function when output is from 1-5 is 'softmax'\n",
    "model.add(Dense(1, activation='linear'))  # Use 'linear' for regression tasks\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62267c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d888c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4cf51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3310b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. DATA QUALITY ASSESSMENT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found in the dataset!\")\n",
    "else:\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate Rows: {duplicates}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# =============================================================================\n",
    "# 3. UNIVARIATE ANALYSIS - DISTRIBUTION OF FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UNIVARIATE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create subplots for all features\n",
    "fig, axes = plt.subplots(7, 3, figsize=(20, 28))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(df.columns):\n",
    "    # Use different shades of blue for histograms\n",
    "    axes[idx].hist(df[col], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    \n",
    "    # Add mean and median lines\n",
    "    mean_val = df[col].mean()\n",
    "    median_val = df[col].median()\n",
    "    axes[idx].axvline(mean_val, color='darkgreen', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[idx].axvline(median_val, color='darkorange', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Check for skewness\n",
    "print(\"\\nSkewness of Features:\")\n",
    "skewness = df.skew()\n",
    "print(skewness.sort_values(ascending=False))\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TARGET VARIABLE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TARGET VARIABLE ANALYSIS (stress_level)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Stress level distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "stress_counts = df['stress_level'].value_counts().sort_index()\n",
    "bars = plt.bar(stress_counts.index, stress_counts.values, edgecolor='black', color='teal', alpha=0.8)\n",
    "plt.xlabel('Stress Level', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Distribution of Stress Levels', fontsize=14, fontweight='bold')\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(stress_counts.values):\n",
    "    plt.text(stress_counts.index[i], v + 5, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "plt.savefig('stress_level_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStress Level Value Counts:\")\n",
    "print(stress_counts)\n",
    "print(f\"\\nPercentage Distribution:\")\n",
    "print((stress_counts / len(df) * 100).round(2))\n",
    "\n",
    "# =============================================================================\n",
    "# 5. CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create a large heatmap for correlation matrix\n",
    "plt.figure(figsize=(16, 14))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "# Use a blue-green colormap\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='YlGnBu', center=0, square=True, linewidths=1,\n",
    "            cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Top correlations with stress_level\n",
    "stress_correlations = correlation_matrix['stress_level'].drop('stress_level').sort_values(ascending=False)\n",
    "print(\"\\nTop 10 Features Correlated with Stress Level:\")\n",
    "print(stress_correlations.head(10))\n",
    "print(\"\\nBottom 10 Features Correlated with Stress Level:\")\n",
    "print(stress_correlations.tail(10))\n",
    "\n",
    "# Visualize correlations with stress_level\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['darkgreen' if x > 0 else 'darkorange' for x in stress_correlations]\n",
    "stress_correlations.plot(kind='barh', color=colors)\n",
    "plt.xlabel('Correlation with Stress Level', fontsize=12)\n",
    "plt.title('Feature Correlations with Stress Level', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('stress_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 6. MULTICOLLINEARITY DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MULTICOLLINEARITY DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:  # Threshold of 0.7\n",
    "            high_corr_pairs.append({\n",
    "                'Feature 1': correlation_matrix.columns[i],\n",
    "                'Feature 2': correlation_matrix.columns[j],\n",
    "                'Correlation': correlation_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', ascending=False)\n",
    "    print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "    print(high_corr_df)\n",
    "else:\n",
    "    print(\"\\nNo highly correlated feature pairs found (threshold: 0.7)\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7. BIVARIATE ANALYSIS - FEATURE VS STRESS LEVEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BIVARIATE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select top features based on correlation\n",
    "top_features = stress_correlations.abs().nlargest(8).index.tolist()\n",
    "\n",
    "# Create box plots for top features vs stress level\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    # Group data by stress level for the current feature\n",
    "    data_to_plot = [df[df['stress_level'] == level][feature].values \n",
    "                    for level in sorted(df['stress_level'].unique())]\n",
    "    \n",
    "    # Create box plot with custom colors\n",
    "    bp = axes[idx].boxplot(data_to_plot, patch_artist=True)\n",
    "    \n",
    "    # Color the boxes with a gradient of blues\n",
    "    colors = plt.cm.Blues(np.linspace(0.4, 0.9, len(bp['boxes'])))\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    # Customize other elements\n",
    "    for element in ['whiskers', 'fliers', 'medians', 'caps']:\n",
    "        plt.setp(bp[element], color='darkblue')\n",
    "    \n",
    "    axes[idx].set_title(f'{feature} vs Stress Level', fontsize=12)\n",
    "    axes[idx].set_xlabel('Stress Level')\n",
    "    axes[idx].set_ylabel(feature)\n",
    "    axes[idx].set_xticklabels(sorted(df['stress_level'].unique()))\n",
    "\n",
    "plt.suptitle('Top Features vs Stress Level', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_vs_stress_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 8. OUTLIER DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OUTLIER DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Detect outliers using IQR method\n",
    "outlier_summary = {}\n",
    "for col in df.columns:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "    outlier_summary[col] = {\n",
    "        'count': len(outliers),\n",
    "        'percentage': (len(outliers) / len(df)) * 100\n",
    "    }\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).T\n",
    "outlier_df = outlier_df.sort_values('count', ascending=False)\n",
    "print(\"\\nOutlier Summary (IQR Method):\")\n",
    "print(outlier_df[outlier_df['count'] > 0])\n",
    "\n",
    "# Visualize outliers\n",
    "plt.figure(figsize=(12, 6))\n",
    "outlier_df[outlier_df['count'] > 0]['percentage'].plot(kind='bar', color='seagreen', alpha=0.8)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Percentage of Outliers', fontsize=12)\n",
    "plt.title('Percentage of Outliers by Feature', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outlier_percentages.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 9. FEATURE IMPORTANCE USING MUTUAL INFORMATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate mutual information scores\n",
    "X = df.drop('stress_level', axis=1)\n",
    "y = df['stress_level']\n",
    "\n",
    "mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "mi_scores_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'MI Score': mi_scores\n",
    "}).sort_values('MI Score', ascending=False)\n",
    "\n",
    "print(\"\\nMutual Information Scores:\")\n",
    "print(mi_scores_df)\n",
    "\n",
    "# Visualize mutual information scores\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Create gradient colors from orange to dark orange\n",
    "colors = plt.cm.Oranges(np.linspace(0.4, 0.9, len(mi_scores_df)))\n",
    "plt.barh(mi_scores_df['Feature'], mi_scores_df['MI Score'], color=colors)\n",
    "plt.xlabel('Mutual Information Score', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Feature Importance based on Mutual Information', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mutual_information_scores.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 10. DIMENSIONALITY REDUCTION VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIMENSIONALITY REDUCTION (PCA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Standardize features for PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Plot explained variance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scree plot\n",
    "ax1.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, 'o-', \n",
    "         color='steelblue', markersize=8, linewidth=2)\n",
    "ax1.set_xlabel('Principal Component', fontsize=12)\n",
    "ax1.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "ax1.set_title('PCA Scree Plot', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative explained variance\n",
    "ax2.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'o-', \n",
    "         color='darkgreen', markersize=8, linewidth=2)\n",
    "ax2.axhline(y=0.95, color='darkorange', linestyle='--', linewidth=2, label='95% Variance')\n",
    "ax2.set_xlabel('Number of Components', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Explained Variance Ratio', fontsize=12)\n",
    "ax2.set_title('Cumulative Explained Variance', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pca_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "print(f\"\\nNumber of components needed for 95% variance: {n_components_95}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 11. STATISTICAL TESTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICAL TESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test for normality of stress_level\n",
    "statistic, p_value = stats.normaltest(df['stress_level'])\n",
    "print(f\"\\nNormality Test for Stress Level:\")\n",
    "print(f\"Statistic: {statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Is normally distributed? {'Yes' if p_value > 0.05 else 'No'}\")\n",
    "\n",
    "# ANOVA test for categorical-like features vs stress level\n",
    "print(\"\\nANOVA Tests (Feature groups by stress level):\")\n",
    "for feature in ['anxiety_level', 'depression', 'academic_performance']:\n",
    "    groups = [group[feature].values for name, group in df.groupby('stress_level')]\n",
    "    f_stat, p_val = stats.f_oneway(*groups)\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"  P-value: {p_val:.4f}\")\n",
    "    print(f\"  Significant difference? {'Yes' if p_val < 0.05 else 'No'}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 12. DATA PREPROCESSING RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDATIONS FOR ML PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. FEATURE SCALING:\")\n",
    "print(\"   - All features are on similar scales (mostly 0-5 range)\")\n",
    "print(\"   - StandardScaler or MinMaxScaler recommended for algorithms sensitive to scale\")\n",
    "print(\"   - Tree-based models may not require scaling\")\n",
    "\n",
    "print(\"\\n2. HANDLING MULTICOLLINEARITY:\")\n",
    "if high_corr_pairs:\n",
    "    print(\"   - Consider removing one feature from highly correlated pairs\")\n",
    "    print(\"   - Or use dimensionality reduction techniques (PCA, LDA)\")\n",
    "else:\n",
    "    print(\"   - No severe multicollinearity detected\")\n",
    "\n",
    "print(\"\\n3. OUTLIER TREATMENT:\")\n",
    "if outlier_df[outlier_df['count'] > 0].shape[0] > 0:\n",
    "    print(\"   - Consider capping outliers or using robust scaling\")\n",
    "    print(\"   - Tree-based models are generally robust to outliers\")\n",
    "else:\n",
    "    print(\"   - Minimal outliers detected\")\n",
    "\n",
    "print(\"\\n4. FEATURE ENGINEERING SUGGESTIONS:\")\n",
    "print(\"   - Create interaction features between highly correlated variables\")\n",
    "print(\"   - Consider polynomial features for non-linear relationships\")\n",
    "print(\"   - Group similar features (e.g., physical symptoms, academic factors)\")\n",
    "\n",
    "print(\"\\n5. CLASS IMBALANCE:\")\n",
    "stress_dist = df['stress_level'].value_counts(normalize=True)\n",
    "if stress_dist.min() < 0.1:\n",
    "    print(\"   - Consider using SMOTE or class weights for imbalanced classes\")\n",
    "else:\n",
    "    print(\"   - Classes are reasonably balanced\")\n",
    "\n",
    "print(\"\\n6. FEATURE SELECTION:\")\n",
    "print(\"   - Use mutual information scores for initial feature selection\")\n",
    "print(\"   - Consider recursive feature elimination with cross-validation\")\n",
    "print(f\"   - Start with top {len(mi_scores_df[mi_scores_df['MI Score'] > 0.1])} features based on MI scores\")\n",
    "\n",
    "# =============================================================================\n",
    "# 13. SAVE ANALYSIS SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "# Create a summary report\n",
    "summary_report = f\"\"\"\n",
    "STRESS LEVEL DATASET ANALYSIS SUMMARY\n",
    "=====================================\n",
    "\n",
    "Dataset Overview:\n",
    "- Total Records: {df.shape[0]}\n",
    "- Total Features: {df.shape[1]}\n",
    "- No Missing Values: {missing_values.sum() == 0}\n",
    "- Duplicate Rows: {duplicates}\n",
    "\n",
    "Target Variable Distribution:\n",
    "{stress_counts.to_dict()}\n",
    "\n",
    "Top 5 Features Correlated with Stress Level:\n",
    "{stress_correlations.head(5).to_dict()}\n",
    "\n",
    "Feature Importance (Top 5 by Mutual Information):\n",
    "{mi_scores_df.head(5).to_dict()}\n",
    "\n",
    "Dimensionality Reduction:\n",
    "- Components for 95% variance: {n_components_95}\n",
    "\n",
    "Outliers Detected:\n",
    "- Features with >5% outliers: {len(outlier_df[outlier_df['percentage'] > 5])}\n",
    "\n",
    "Preprocessing Recommendations:\n",
    "1. Scaling: Recommended (StandardScaler/MinMaxScaler)\n",
    "2. Feature Selection: Start with top {len(mi_scores_df[mi_scores_df['MI Score'] > 0.1])} features\n",
    "3. Handle multicollinearity if needed\n",
    "4. Consider ensemble methods for robustness\n",
    "\"\"\"\n",
    "\n",
    "# Save summary to file\n",
    "with open('analysis_summary.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"- feature_distributions.png\")\n",
    "print(\"- stress_level_distribution.png\")\n",
    "print(\"- correlation_matrix.png\")\n",
    "print(\"- stress_correlations.png\")\n",
    "print(\"- feature_vs_stress_boxplots.png\")\n",
    "print(\"- outlier_percentages.png\")\n",
    "print(\"- mutual_information_scores.png\")\n",
    "print(\"- pca_analysis.png\")\n",
    "print(\"- analysis_summary.txt\")\n",
    "print(\"\\nReady for ML modeling!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
